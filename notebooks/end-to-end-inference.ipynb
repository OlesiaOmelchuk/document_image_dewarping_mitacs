{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12865864,"sourceType":"datasetVersion","datasetId":8138261},{"sourceId":12962518,"sourceType":"datasetVersion","datasetId":8138187},{"sourceId":258930833,"sourceType":"kernelVersion"},{"sourceId":557814,"sourceType":"modelInstanceVersion","modelInstanceId":423947,"modelId":441464},{"sourceId":557817,"sourceType":"modelInstanceVersion","modelInstanceId":423949,"modelId":441464}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing dependencies","metadata":{}},{"cell_type":"code","source":"!apt install tesseract-ocr libtesseract-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:52:50.948098Z","iopub.execute_input":"2025-09-04T15:52:50.949177Z","iopub.status.idle":"2025-09-04T15:52:54.762745Z","shell.execute_reply.started":"2025-09-04T15:52:50.949141Z","shell.execute_reply":"2025-09-04T15:52:54.761133Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibtesseract-dev is already the newest version (4.1.1-2.1build1).\ntesseract-ocr is already the newest version (4.1.1-2.1build1).\n0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"!pip install hdf5storage Levenshtein jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:52:54.766242Z","iopub.execute_input":"2025-09-04T15:52:54.766577Z","iopub.status.idle":"2025-09-04T15:52:59.085793Z","shell.execute_reply.started":"2025-09-04T15:52:54.766549Z","shell.execute_reply":"2025-09-04T15:52:59.084246Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: hdf5storage in /usr/local/lib/python3.11/dist-packages (0.2.1)\nRequirement already satisfied: Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\nRequirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\nRequirement already satisfied: h5py>=3.9 in /usr/local/lib/python3.11/dist-packages (from hdf5storage) (3.14.0)\nRequirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.11/dist-packages (from hdf5storage) (1.26.4)\nRequirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein) (3.14.0)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26->hdf5storage) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26->hdf5storage) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26->hdf5storage) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26->hdf5storage) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26->hdf5storage) (2024.2.0)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"!pip install -q git+https://github.com/allansdefreitas/yolov10.git\n!pip install -q supervision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:52:59.087448Z","iopub.execute_input":"2025-09-04T15:52:59.087873Z","iopub.status.idle":"2025-09-04T15:53:15.580347Z","shell.execute_reply.started":"2025-09-04T15:52:59.087826Z","shell.execute_reply":"2025-09-04T15:53:15.578940Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!wget https://github.com/moured/YOLOv10-Document-Layout-Analysis/releases/download/doclaynet_weights/yolov10x_best.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:15.582258Z","iopub.execute_input":"2025-09-04T15:53:15.582604Z","iopub.status.idle":"2025-09-04T15:53:16.458269Z","shell.execute_reply.started":"2025-09-04T15:53:15.582571Z","shell.execute_reply":"2025-09-04T15:53:16.457032Z"}},"outputs":[{"name":"stdout","text":"--2025-09-04 15:53:15--  https://github.com/moured/YOLOv10-Document-Layout-Analysis/releases/download/doclaynet_weights/yolov10x_best.pt\nResolving github.com (github.com)... 140.82.113.3\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://release-assets.githubusercontent.com/github-production-release-asset/809399250/e52eefec-ac07-4944-997c-59e48e23474b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-04T16%3A35%3A13Z&rscd=attachment%3B+filename%3Dyolov10x_best.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-04T15%3A34%3A27Z&ske=2025-09-04T16%3A35%3A13Z&sks=b&skv=2018-11-09&sig=DxuqihqLlPcLPYWQ9DqaCnUm8EmQFuOFfEhBKOqnqu0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NzAwMTQ5NSwibmJmIjoxNzU3MDAxMTk1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ._1ubQBlqtYBJyzRhKlobroAlQDGAmVGshQL_SygrS94&response-content-disposition=attachment%3B%20filename%3Dyolov10x_best.pt&response-content-type=application%2Foctet-stream [following]\n--2025-09-04 15:53:15--  https://release-assets.githubusercontent.com/github-production-release-asset/809399250/e52eefec-ac07-4944-997c-59e48e23474b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-04T16%3A35%3A13Z&rscd=attachment%3B+filename%3Dyolov10x_best.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-04T15%3A34%3A27Z&ske=2025-09-04T16%3A35%3A13Z&sks=b&skv=2018-11-09&sig=DxuqihqLlPcLPYWQ9DqaCnUm8EmQFuOFfEhBKOqnqu0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NzAwMTQ5NSwibmJmIjoxNzU3MDAxMTk1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ._1ubQBlqtYBJyzRhKlobroAlQDGAmVGshQL_SygrS94&response-content-disposition=attachment%3B%20filename%3Dyolov10x_best.pt&response-content-type=application%2Foctet-stream\nResolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 64133947 (61M) [application/octet-stream]\nSaving to: ‘yolov10x_best.pt.1’\n\nyolov10x_best.pt.1  100%[===================>]  61.16M   134MB/s    in 0.5s    \n\n2025-09-04 15:53:16 (134 MB/s) - ‘yolov10x_best.pt.1’ saved [64133947/64133947]\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"!pip install reportlab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:16.461647Z","iopub.execute_input":"2025-09-04T15:53:16.462047Z","iopub.status.idle":"2025-09-04T15:53:20.570306Z","shell.execute_reply.started":"2025-09-04T15:53:16.462011Z","shell.execute_reply":"2025-09-04T15:53:20.569039Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: reportlab in /usr/local/lib/python3.11/dist-packages (4.4.3)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.2.1)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab) (3.4.2)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"# Model & Inference Code","metadata":{}},{"cell_type":"markdown","source":"## Dewarping Model Architecture (Flow Generator)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nimport hdf5storage as h5\nimport cv2\nimport numpy as np\nfrom einops import rearrange\nimport time\nimport argparse\nfrom torchvision import transforms\nimport torchvision.utils as vutils\nimport wandb  # Optional for logging\nimport matplotlib.pyplot as plt\n\n\n# ---------------------------\n# Model Architecture (Your Transformer+U-Net)\n# ---------------------------\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, heads, mlp_ratio=4.0, p=0.0):\n        super().__init__()\n        self.n1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False)\n        self.n2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n        )\n    def forward(self, x):  # x: [HW,B,D]\n        h = self.n1(x)\n        a, _ = self.attn(h, h, h, need_weights=False)\n        x = x + a\n        x = x + self.mlp(self.n2(x))\n        return x\n\nclass MultiStageTransformerEncoder(nn.Module):\n    def __init__(self, img_channels=3, embed_dims=[64,128,256], patch_sizes=[8,16,2], depths=[2,2,2], heads=[2,4,8]):\n        super().__init__()\n        self.stages = nn.ModuleList()\n        self.embed_dims = embed_dims\n        for i, d in enumerate(embed_dims):\n            in_ch = img_channels if i == 0 else embed_dims[i-1]\n            self.stages.append(nn.ModuleDict({\n                \"proj\": nn.Conv2d(in_ch, d, kernel_size=patch_sizes[i], stride=patch_sizes[i]),\n                \"blocks\": nn.ModuleList([TransformerBlock(d, heads[i]) for _ in range(depths[i])])\n            }))\n    def forward(self, x):\n        skips = []\n        for s in self.stages:\n            x = s[\"proj\"](x)             # [B,D,h,w]\n            B, D, h, w = x.shape\n            x_seq = rearrange(x, \"b d h w -> (h w) b d\")\n            for blk in s[\"blocks\"]:\n                x_seq = blk(x_seq)\n            x = rearrange(x_seq, \"(h w) b d -> b d h w\", h=h, w=w)\n            skips.append(x)\n        return skips  # [low-res ... high-res]\n\nclass UNetDecoder(nn.Module):\n    def __init__(self, embed_dims=[64,128,256], out_ch=2):\n        super().__init__()\n        self.up1 = nn.ConvTranspose2d(embed_dims[2], embed_dims[1], 2, 2)\n        self.c1  = nn.Sequential(nn.Conv2d(embed_dims[1]*2, embed_dims[1], 3, padding=1), nn.ReLU(True),\n                                 nn.Conv2d(embed_dims[1], embed_dims[1], 3, padding=1), nn.ReLU(True))\n        self.up2 = nn.ConvTranspose2d(embed_dims[1], embed_dims[0], 2, 2)\n        self.c2  = nn.Sequential(nn.Conv2d(embed_dims[0]*2, embed_dims[0], 3, padding=1), nn.ReLU(True),\n                                 nn.Conv2d(embed_dims[0], embed_dims[0], 3, padding=1), nn.ReLU(True))\n        self.up3 = nn.ConvTranspose2d(embed_dims[0], embed_dims[0]//2, 2, 2)\n        self.c3  = nn.Sequential(nn.Conv2d(embed_dims[0]//2, embed_dims[0]//2, 3, padding=1), nn.ReLU(True))\n        self.out = nn.Conv2d(embed_dims[0]//2, out_ch, 1)\n    def forward(self, skips):\n        x = skips[-1]\n        x = self.up1(x)\n        s1 = F.interpolate(skips[1], size=x.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, s1], dim=1); x = self.c1(x)\n\n        x = self.up2(x)\n        s0 = F.interpolate(skips[0], size=x.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, s0], dim=1); x = self.c2(x)\n\n        x = self.up3(x); x = self.c3(x)\n        return self.out(x)\n\nclass FlowGenerator(nn.Module):\n    \"\"\"Predicts flow to transform input between domains.\"\"\"\n    def __init__(self, img_channels=3, max_disp=48.0):\n        super().__init__()\n        self.enc = MultiStageTransformerEncoder(img_channels=img_channels)\n        self.dec = UNetDecoder()\n        self.max_disp = max_disp\n    def forward(self, x):\n        B, C, H, W = x.shape\n        skips = self.enc(x)\n        flow = self.dec(skips)\n        flow = F.interpolate(flow, size=(H, W), mode='bilinear', align_corners=False)\n        # constrain displacement magnitude for stability\n        # flow = torch.tanh(flow) * self.max_disp\n        flow = flow * 10.0  # EXPERIMENT: 10x amplification for visibility\n        return flow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:20.572069Z","iopub.execute_input":"2025-09-04T15:53:20.572504Z","iopub.status.idle":"2025-09-04T15:53:20.619440Z","shell.execute_reply.started":"2025-09-04T15:53:20.572459Z","shell.execute_reply":"2025-09-04T15:53:20.618036Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"## Dewarping inference funcs","metadata":{}},{"cell_type":"code","source":"def apply_bm_doc3d(img, bm_pix, align_corners=True, padding_mode=\"border\", verbose=False, save_path=None):\n    \"\"\"\n    Warp an image using a backward map in pixel coordinates.\n\n    Args:\n        img: (B, C, H, W) tensor in [0,1], warped image\n        bm_pix: (B, 2, H, W) tensor in pixels, backward map (absolute coords)\n                bm_pix[:,0] = x pixel coords\n                bm_pix[:,1] = y pixel coords\n        align_corners: bool, matches normalization convention in grid_sample\n        padding_mode: str, 'border' or 'zeros'\n\n    Returns:\n        rectified: (B, C, H, W) tensor, unwarped image\n    \"\"\"        \n    B, C, H, W = img.shape\n\n    # convert pixel coords -> normalized [-1,1]\n    if align_corners:\n        norm_x = (bm_pix[:, 0, :, :] / (W - 1)) * 2 - 1\n        norm_y = (bm_pix[:, 1, :, :] / (H - 1)) * 2 - 1\n    else:\n        norm_x = (2 * bm_pix[:, 0, :, :] + 1) / W - 1\n        norm_y = (2 * bm_pix[:, 1, :, :] + 1) / H - 1\n\n    grid = torch.stack([norm_x, norm_y], dim=-1)  # (B,H,W,2)\n\n    rectified = F.grid_sample(\n        img, grid, mode=\"bilinear\",\n        padding_mode=padding_mode, align_corners=align_corners\n    )\n        \n    if verbose:\n        img_display = prepare_tensor(img)\n        rectified_display = prepare_tensor(rectified)\n        \n        f,axrr=plt.subplots(1,2)\n        for ax in axrr:\n            ax.set_xticks([])\n            ax.set_yticks([])\n        axrr[0].imshow(img_display)\n        axrr[0].title.set_text('input')\n        axrr[1].imshow(rectified_display)\n        axrr[1].title.set_text('unwarped')\n        if save_path is not None:\n            plt.savefig(save_path)\n        plt.show()\n        \n    return rectified\n\n\ndef scale_flow_to_resolution(flow_low_res, low_res, high_res):\n    \"\"\"\n    Scale flow from low resolution to high resolution\n    flow_low_res: [1, 2, H_low, W_low]\n    low_res: (H_low, W_low)\n    high_res: (H_high, W_high)\n    \"\"\"\n    if low_res == high_res:\n        return flow_low_res\n        \n    H_low, W_low = low_res\n    H_high, W_high = high_res\n    \n    # Scale factors\n    scale_x = W_high / W_low\n    scale_y = H_high / H_low\n    \n    # Resize flow\n    flow_high_res = F.interpolate(flow_low_res, size=(H_high, W_high), \n                                 mode='bilinear', align_corners=False)\n    \n    # Scale flow values\n    flow_high_res[:, 0, :, :] *= scale_x  # x coordinates\n    flow_high_res[:, 1, :, :] *= scale_y  # y coordinates\n    \n    return flow_high_res\n    \n    \ndef dewarp_high_res_scaled(model, high_res_img_path, device, out_path=None, target_size=(448, 448)):\n    \"\"\"\n    (Main inference function)\n    Dewarp by scaling flow to high resolution\n    \"\"\"\n    # Load high-res image\n    high_res_img = cv2.imread(high_res_img_path)\n    high_res_img = cv2.cvtColor(high_res_img, cv2.COLOR_BGR2RGB)\n    original_size = high_res_img.shape[:2]  # (H, W)\n    \n    # Resize to training size for prediction\n    img_resized = cv2.resize(high_res_img, target_size)\n    img_tensor = torch.from_numpy(img_resized).float().permute(2, 0, 1) / 255.0\n    img_tensor = img_tensor.unsqueeze(0).to(device)\n    \n    # Predict flow on resized image\n    with torch.no_grad():\n        flow_low_res = model(img_tensor)  # [1, 2, 448, 448]\n    \n    # Scale flow to original resolution\n    flow_high_res = scale_flow_to_resolution(flow_low_res, target_size, original_size)\n    \n    # Prepare high-res image tensor\n    img_high_res_tensor = torch.from_numpy(high_res_img).float().permute(2, 0, 1) / 255.0\n    img_high_res_tensor = img_high_res_tensor.unsqueeze(0).to(device)\n    \n    # Apply scaled flow to high-res image\n    dewarped_high_res = apply_bm_doc3d(img_high_res_tensor, flow_high_res, align_corners=True)\n    \n    # Save\n    if out_path is not None:\n        save_dewarped_result(dewarped_high_res, out_path)\n\n    return dewarped_high_res, high_res_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:20.620678Z","iopub.execute_input":"2025-09-04T15:53:20.621059Z","iopub.status.idle":"2025-09-04T15:53:20.651066Z","shell.execute_reply.started":"2025-09-04T15:53:20.621028Z","shell.execute_reply":"2025-09-04T15:53:20.650123Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Visualzation helper funcs","metadata":{}},{"cell_type":"code","source":"from io import BytesIO\nfrom PIL import Image\n\n\ndef prepare_tensor(tensor):\n    if isinstance(tensor, np.ndarray):\n        if tensor.ndim == 4:\n            tensor = tensor[0]  # remove batch dimension\n        if tensor.shape[0] in [1, 3, 4]:  # channels first\n            tensor = tensor.transpose(1, 2, 0)  # convert to channels last\n        return tensor\n    if tensor.requires_grad:\n        tensor = tensor.detach()\n    tensor = tensor.cpu().numpy()\n    if tensor.ndim == 4:\n        tensor = tensor[0]\n    if tensor.shape[0] in [1, 3, 4]:\n        tensor = tensor.transpose(1, 2, 0)\n    return tensor\n\ndef visualize_flow(flow, save_path=None, verbose=True):\n    \"\"\"Visualize flow field as RGB image\"\"\"\n    try:\n        flow = flow.squeeze(0).cpu().numpy()\n    except:\n        flow = flow.squeeze(0)\n    \n    # Convert flow to HSV color representation\n    h, w = flow.shape[1:]\n    hsv = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    # Magnitude and angle\n    mag, ang = cv2.cartToPolar(flow[0], flow[1])\n    \n    # Normalize for visualization\n    hsv[..., 0] = ang * 180 / np.pi / 2\n    hsv[..., 1] = 255\n    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n    \n    # Convert to BGR and save\n    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n    \n    if save_path is not None:\n        cv2.imwrite(save_path, bgr)\n\n    # Create plot\n    if verbose:\n        plt.figure(figsize=(10, 8))\n        plt.imshow(bgr)\n        plt.axis('off')\n        plt.title('Optical Flow Visualization')\n        plt.tight_layout()\n        plt.show()\n        \n    return bgr\n\ndef save_dewarped_result(dewarped_tensor, out_path, clip=True, verbose=False):\n    \"\"\"\n    Save a dewarped image tensor to file\n    \n    Args:\n        dewarped_tensor: torch.Tensor of shape [1, 3, H, W] or [3, H, W] in range [0, 1]\n        out_path: path to save the image\n        clip: whether to clip values to [0, 1] range\n        verbose: whether to print debug information\n    \"\"\"\n    # Ensure we're working with CPU numpy array\n    if hasattr(dewarped_tensor, 'detach'):\n        dewarped_tensor = dewarped_tensor.detach()\n    if hasattr(dewarped_tensor, 'cpu'):\n        dewarped_tensor = dewarped_tensor.cpu()\n    if hasattr(dewarped_tensor, 'numpy'):\n        dewarped_tensor = dewarped_tensor.numpy()\n    \n    # Handle different tensor shapes\n    if dewarped_tensor.ndim == 4:  # [B, C, H, W]\n        dewarped_tensor = dewarped_tensor[0]  # Take first batch\n    if dewarped_tensor.ndim == 3:  # [C, H, W]\n        dewarped_tensor = dewarped_tensor.transpose(1, 2, 0)  # [H, W, C]\n    \n    # Clip values to valid range\n    if clip:\n        dewarped_tensor = np.clip(dewarped_tensor, 0, 1)\n    \n    # Convert to 8-bit and change channel order for OpenCV\n    dewarped_image = (dewarped_tensor * 255).astype(np.uint8)\n    dewarped_image = cv2.cvtColor(dewarped_image, cv2.COLOR_RGB2BGR)\n    \n    # Save image\n    success = cv2.imwrite(out_path, dewarped_image)\n    \n    if verbose:\n        print(f\"Saved dewarped image to: {out_path}\")\n        print(f\"Image shape: {dewarped_image.shape}\")\n        print(f\"Value range: [{dewarped_tensor.min():.3f}, {dewarped_tensor.max():.3f}]\")\n        print(f\"Save successful: {success}\")\n    \n    return success","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:20.652323Z","iopub.execute_input":"2025-09-04T15:53:20.652641Z","iopub.status.idle":"2025-09-04T15:53:20.677288Z","shell.execute_reply.started":"2025-09-04T15:53:20.652616Z","shell.execute_reply":"2025-09-04T15:53:20.676043Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# OCR Code","metadata":{}},{"cell_type":"code","source":"import pytesseract\nfrom PIL import Image\nimport Levenshtein as lv\nfrom jiwer import cer\n\n\n# TODO: extend to add the ability for bounding boxes extraction\ndef extract_OCR_text(filepath):\n    with Image.open(filepath) as img:\n        OCR_text = pytesseract.image_to_string(img, config=\"--oem 1\")\n    return OCR_text\n\ndef calculate_OCR_metrics(GT_text, OCR_text):\n    CER = cer(GT_text, OCR_text)\n    ED = lv.distance(OCR_text, GT_text)\n    return {\n        'CER': CER,\n        'ED': ED\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:20.678469Z","iopub.execute_input":"2025-09-04T15:53:20.678844Z","iopub.status.idle":"2025-09-04T15:53:20.706019Z","shell.execute_reply.started":"2025-09-04T15:53:20.678815Z","shell.execute_reply":"2025-09-04T15:53:20.705040Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Layout Analysis Code","metadata":{}},{"cell_type":"code","source":"import supervision as sv\nfrom ultralytics import YOLOv10\n\n\ndoc_layout_model = YOLOv10('yolov10x_best.pt')\n\ndef detect_img_layout(path, verbose=False):\n    \"\"\"\n    names: {0: 'Caption', 1: 'Footnote', 2: 'Formula', 3: 'List-item', 4: 'Page-footer', 5: 'Page-header', \n            6: 'Picture', 7: 'Section-header', 8: 'Table', 9: 'Text', 10: 'Title'}\n    \"\"\"\n    image = cv2.imread(path)\n    results = doc_layout_model(source=path, conf=0.2, iou=0.8)[0]\n    detections = sv.Detections.from_ultralytics(results)\n\n    if verbose == True:\n        bounding_box_annotator = sv.BoxAnnotator()\n        label_annotator = sv.LabelAnnotator()\n        \n        annotated_image = bounding_box_annotator.annotate(\n            scene=image, detections=detections)\n        annotated_image = label_annotator.annotate(\n            scene=annotated_image, detections=detections)\n    \n        sv.plot_image(annotated_image)\n\n    return detections","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:53:20.707181Z","iopub.execute_input":"2025-09-04T15:53:20.707643Z","iopub.status.idle":"2025-09-04T15:53:21.073584Z","shell.execute_reply.started":"2025-09-04T15:53:20.707609Z","shell.execute_reply":"2025-09-04T15:53:21.072534Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"markdown","source":"## Loading trained models","metadata":{}},{"cell_type":"code","source":"config = {\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'max_disp': 48.0,\n}\n\nckpt_path_dict = {\n    'kaggle_5k_98ep': '/kaggle/input/supervised-dewarping-training/checkpoints/checkpoint_epoch_98.pth',\n    'cc_100k_8ep': '/kaggle/input/supervised_dewarping_model/pytorch/epoch-8/1/checkpoint_epoch_8.pth',\n    'cc_100k_24ep': '/kaggle/input/supervised_dewarping_model/pytorch/default-epoch-24/1/checkpoint_epoch_24.pth'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:05:58.700808Z","iopub.execute_input":"2025-09-04T14:05:58.701538Z","iopub.status.idle":"2025-09-04T14:05:58.706845Z","shell.execute_reply.started":"2025-09-04T14:05:58.701504Z","shell.execute_reply":"2025-09-04T14:05:58.705806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = config['device']\nmax_disp = config['max_disp']\n\nprint(f'Using device: {device}, max_disp: {max_disp}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:05:58.707891Z","iopub.execute_input":"2025-09-04T14:05:58.708248Z","iopub.status.idle":"2025-09-04T14:05:58.730602Z","shell.execute_reply.started":"2025-09-04T14:05:58.708217Z","shell.execute_reply":"2025-09-04T14:05:58.729300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models_dict = {}\n\nfor ckpt, path in ckpt_path_dict.items():\n    model = FlowGenerator(max_disp=max_disp).to(device)\n    checkpoint = torch.load(path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    models_dict[ckpt] = model\n\nmodels_dict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:05:58.731687Z","iopub.execute_input":"2025-09-04T14:05:58.731974Z","iopub.status.idle":"2025-09-04T14:06:02.778266Z","shell.execute_reply.started":"2025-09-04T14:05:58.731936Z","shell.execute_reply":"2025-09-04T14:06:02.777304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference testing","metadata":{}},{"cell_type":"code","source":"img_paths = ['/kaggle/input/dir300/DIR300/dist/1.png', '/kaggle/input/dir300/DIR300/dist/100.png', '/kaggle/input/dir300/DIR300/dist/106.png', '/kaggle/input/dir300/DIR300/dist/103.png', '/kaggle/input/dir300/DIR300/dist/12.png']\n\nfor path in img_paths:\n    print(path)\n    display(Image.open(path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:06:02.779373Z","iopub.execute_input":"2025-09-04T14:06:02.779906Z","iopub.status.idle":"2025-09-04T14:06:15.182169Z","shell.execute_reply.started":"2025-09-04T14:06:02.779875Z","shell.execute_reply":"2025-09-04T14:06:15.180182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for warped_img_path in img_paths:\n    print(warped_img_path)\n    for name, model in models_dict.items():\n        dewarped, high_res_img = dewarp_high_res_scaled(model, warped_img_path, device, out_path=f\"{os.path.basename(warped_img_path)[:-4]}-{name}.png\")\n\n        if name == 'kaggle_5k_98ep':\n            plt.figure(figsize=(10, 8))\n            plt.imshow(prepare_tensor(high_res_img))\n            plt.axis('off')\n            plt.show()\n\n        print(name)\n        \n        plt.figure(figsize=(10, 8))\n        plt.imshow(prepare_tensor(dewarped))\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:06:15.183797Z","iopub.execute_input":"2025-09-04T14:06:15.184514Z","iopub.status.idle":"2025-09-04T14:07:41.115174Z","shell.execute_reply.started":"2025-09-04T14:06:15.184459Z","shell.execute_reply":"2025-09-04T14:07:41.113809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## OCR","metadata":{}},{"cell_type":"code","source":"gt_img_paths = ['/kaggle/input/dir300/DIR300/gt/1.png', '/kaggle/input/dir300/DIR300/gt/100.png', '/kaggle/input/dir300/DIR300/gt/106.png', '/kaggle/input/dir300/DIR300/gt/103.png', '/kaggle/input/dir300/DIR300/gt/12.png']\n\nfor warped_img_path, gt_img_path in zip(img_paths, gt_img_paths):\n    print(warped_img_path, gt_img_path)\n    gt_text = extract_OCR_text(gt_img_path)\n    warped_text = extract_OCR_text(warped_img_path)\n    metrics = calculate_OCR_metrics(gt_text, warped_text)\n    print(\"GT vs Warped:\", metrics)\n    for name, model in models_dict.items():\n        dewarped_img_path = f\"{os.path.basename(warped_img_path)[:-4]}-{name}.png\"\n        dewarped_text = extract_OCR_text(dewarped_img_path)\n        metrics = calculate_OCR_metrics(gt_text, dewarped_text)\n        print(f\"GT vs {name}: {metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:07:41.116931Z","iopub.execute_input":"2025-09-04T14:07:41.117762Z","iopub.status.idle":"2025-09-04T14:11:08.855718Z","shell.execute_reply.started":"2025-09-04T14:07:41.117722Z","shell.execute_reply":"2025-09-04T14:11:08.854650Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Layout analysis ","metadata":{}},{"cell_type":"code","source":"gt_img_paths = ['/kaggle/input/dir300/DIR300/gt/1.png', '/kaggle/input/dir300/DIR300/gt/100.png', '/kaggle/input/dir300/DIR300/gt/106.png', '/kaggle/input/dir300/DIR300/gt/103.png', '/kaggle/input/dir300/DIR300/gt/12.png']\n\nfor warped_img_path, gt_img_path in zip(img_paths, gt_img_paths):\n    print(warped_img_path, gt_img_path)\n    detect_img_layout(gt_img_path, verbose=True)\n    detect_img_layout(warped_img_path, verbose=True)\n    for name, model in models_dict.items():\n        dewarped_img_path = f\"{os.path.basename(warped_img_path)[:-4]}-{name}.png\"\n        detect_img_layout(dewarped_img_path, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:11:08.856868Z","iopub.execute_input":"2025-09-04T14:11:08.857339Z","iopub.status.idle":"2025-09-04T14:12:37.641721Z","shell.execute_reply.started":"2025-09-04T14:11:08.857297Z","shell.execute_reply":"2025-09-04T14:12:37.640730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PDF generation","metadata":{}},{"cell_type":"markdown","source":"## Text only","metadata":{}},{"cell_type":"code","source":"from reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\nfrom IPython.display import IFrame\n\ndef text_to_pdf(text, filename=\"output.pdf\"):\n    # Create a canvas with letter page size\n    c = canvas.Canvas(filename, pagesize=letter)\n    width, height = letter\n\n    # Set font and starting position\n    c.setFont(\"Helvetica\", 12)\n    x, y = 72, height - 72  # 1-inch margin\n\n    # Write text line by line (auto-wrap for long strings)\n    for line in text.split(\"\\n\"):\n        c.drawString(x, y, line)\n        y -= 15  # move down for next line\n        if y < 72:  # start a new page if out of space\n            c.showPage()\n            c.setFont(\"Helvetica\", 12)\n            y = height - 72\n\n    # Save the PDF\n    c.save()\n    print(f\"PDF saved as {filename}\")\n\n\ndef img_to_pdf(img_path, pdf_path, verbose=True):\n    if verbose:\n        display(Image.open(img_path))\n        \n    text = extract_OCR_text(img_path)\n    text_to_pdf(gt_text, pdf_path)\n    if verbose:\n        display(IFrame(pdf_path, width=600, height=400))\n        \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:18:37.037338Z","iopub.execute_input":"2025-09-04T14:18:37.038114Z","iopub.status.idle":"2025-09-04T14:18:37.045598Z","shell.execute_reply.started":"2025-09-04T14:18:37.038082Z","shell.execute_reply":"2025-09-04T14:18:37.044530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_to_pdf('/kaggle/input/dir300/DIR300/gt/1.png', 'text_only-gt_1.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:18:39.817124Z","iopub.execute_input":"2025-09-04T14:18:39.817455Z","iopub.status.idle":"2025-09-04T14:18:43.892274Z","shell.execute_reply.started":"2025-09-04T14:18:39.817435Z","shell.execute_reply":"2025-09-04T14:18:43.891459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_to_pdf('/kaggle/input/dir300/DIR300/gt/100.png', 'text_only-gt_100.pdf')\nimg_to_pdf('/kaggle/input/dir300/DIR300/dist/100.png', 'text_only-dist_100.pdf')\nimg_to_pdf('/kaggle/working/100-cc_100k_24ep.png', 'text_only-100-cc_100k_24ep.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:18:47.857078Z","iopub.execute_input":"2025-09-04T14:18:47.857755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation benchmarks","metadata":{}},{"cell_type":"markdown","source":"## DIR300","metadata":{}},{"cell_type":"code","source":"# Source (original authors of the DIR300 dataset): https://github.com/fh2019ustc/DocGeoNet/blob/main/OCR_eval_DIR300.py\n\ndef Levenshtein_Distance(str1, str2):\n    matrix = [[ i + j for j in range(len(str2) + 1)] for i in range(len(str1) + 1)]\n    for i in range(1, len(str1)+1):\n        for j in range(1, len(str2)+1):\n            if(str1[i-1] == str2[j-1]):\n                d = 0\n            else:\n                d = 1 \n            matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+d)\n\n    return matrix[len(str1)][len(str2)]\n\ndef cal_cer_ed(path_ours, tail='_rec'):\n    path_gt='/kaggle/input/dir300/DIR300 FULL/gt/'\n    cer1=[]\n    ed1=[]\n    lis=[5,7,8,10,12,27,28,29,31,36,53,55,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,85,94,96]+\\\n         [103,107,108,111,115,126,128,129,130,133,135,139,140,148,149,151,159,160,161,162,163,164,165,166,167,169,170,173,174,177]+\\\n         [201,202,203,205,217,218,222,223,225,227,228,237,238,239,264,265,266,271,273,277,278,285,286,288,291,294,295,296,298,300]  # 90 images in DIR300\n    print(len(lis))\n    for i in range(1,301):\n        if i not in lis:\n            continue\n        if not os.path.exists(path_gt+str(i)+'.png'):\n            print(path_gt+str(i)+'.png')\n        if not os.path.exists(path_ours+str(i) + tail):\n            print(path_ours+str(i) + tail)\n        # gt=Image.open(path_gt+str(i)+'.png')\n        # img1=Image.open(path_ours+str(i) + tail)\n        # content_gt=pytesseract.image_to_string(gt)\n        # content1=pytesseract.image_to_string(img1)\n        # l1=Levenshtein_Distance(content_gt,content1)\n        # ed1.append(l1)\n        # cer1.append(l1/len(content_gt))\n    print('CER: ', np.mean(cer1))\n    print('ED:  ', np.mean(ed1))\n\ndef evalu(path_ours, tail):\n    cal_cer_ed(path_ours, tail)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:58:58.204940Z","iopub.execute_input":"2025-09-04T16:58:58.205305Z","iopub.status.idle":"2025-09-04T16:58:58.218149Z","shell.execute_reply.started":"2025-09-04T16:58:58.205279Z","shell.execute_reply":"2025-09-04T16:58:58.217045Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"evalu(\"/kaggle/input/dir300/DIR300 FULL/dist/\", \".png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T16:59:00.443142Z","iopub.execute_input":"2025-09-04T16:59:00.443488Z","iopub.status.idle":"2025-09-04T16:59:00.662834Z","shell.execute_reply.started":"2025-09-04T16:59:00.443463Z","shell.execute_reply":"2025-09-04T16:59:00.661802Z"}},"outputs":[{"name":"stdout","text":"90\nCER:  nan\nED:   nan\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport pytesseract\nimport numpy as np\n\ndef cal_cer_ed(path_ours, tail='_rec', save_text=True, output_dir='extracted_text'):\n    path_gt = '/kaggle/input/dir300/DIR300 FULL/gt/'\n    cer1 = []\n    ed1 = []\n    lis = [5,7,8,10,12,27,28,29,31,36,53,55,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,85,94,96] + \\\n         [103,107,108,111,115,126,128,129,130,133,135,139,140,148,149,151,159,160,161,162,163,164,165,166,167,169,170,173,174,177] + \\\n         [201,202,203,205,217,218,222,223,225,227,228,237,238,239,264,265,266,271,273,277,278,285,286,288,291,294,295,296,298,300]  # 90 images in DIR300\n    \n    print(len(lis))\n    \n    # Create output directory if it doesn't exist and we want to save text\n    if save_text and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    for i in range(1, 301):\n        if i not in lis:\n            continue\n        \n        gt = Image.open(path_gt + str(i) + '.png')\n        img1 = Image.open(path_ours + str(i) + tail)\n        \n        content_gt = pytesseract.image_to_string(gt)\n        content1 = pytesseract.image_to_string(img1)\n        \n        # Save extracted text to files if requested\n        if save_text:\n            # Save ground truth text\n            gt_filename = os.path.join(output_dir, f'gt_{i}.txt')\n            with open(gt_filename, 'w', encoding='utf-8') as f:\n                f.write(content_gt)\n            \n            # Save our method's text\n            our_filename = os.path.join(output_dir, f'our_{i}.txt')\n            with open(our_filename, 'w', encoding='utf-8') as f:\n                f.write(content1)\n        \n        l1 = Levenshtein_Distance(content_gt, content1)\n        ed1.append(l1)\n        cer1.append(l1 / len(content_gt))\n    \n    print('CER: ', np.mean(cer1))\n    print('ED:  ', np.mean(ed1))\n    \n    # Return the results for further analysis if needed\n    return cer1, ed1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:00:10.339071Z","iopub.execute_input":"2025-09-04T17:00:10.339377Z","iopub.status.idle":"2025-09-04T17:00:10.351558Z","shell.execute_reply.started":"2025-09-04T17:00:10.339355Z","shell.execute_reply":"2025-09-04T17:00:10.350435Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"cer_results, ed_results = cal_cer_ed('/kaggle/input/dir300/DIR300 FULL/dist/', '.png', output_dir='text_results')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T17:00:12.785772Z","iopub.execute_input":"2025-09-04T17:00:12.786155Z"}},"outputs":[{"name":"stdout","text":"90\n","output_type":"stream"}],"execution_count":null}]}