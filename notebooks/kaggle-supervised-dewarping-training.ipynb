{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:31.380298Z",
     "iopub.status.busy": "2025-09-02T02:46:31.379972Z",
     "iopub.status.idle": "2025-09-02T02:46:34.400935Z",
     "shell.execute_reply": "2025-09-02T02:46:34.400227Z",
     "shell.execute_reply.started": "2025-09-02T02:46:31.380273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.403123Z",
     "iopub.status.busy": "2025-09-02T02:46:34.402411Z",
     "iopub.status.idle": "2025-09-02T02:46:34.420287Z",
     "shell.execute_reply": "2025-09-02T02:46:34.419511Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.403096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import hdf5storage as h5\n",
    "import cv2\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import time\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import wandb  # Optional for logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model Architecture (Your Transformer+U-Net)\n",
    "# ---------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4.0, p=0.0):\n",
    "        super().__init__()\n",
    "        self.n1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False)\n",
    "        self.n2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "    def forward(self, x):  # x: [HW,B,D]\n",
    "        h = self.n1(x)\n",
    "        a, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + a\n",
    "        x = x + self.mlp(self.n2(x))\n",
    "        return x\n",
    "\n",
    "class MultiStageTransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_channels=3, embed_dims=[64,128,256], patch_sizes=[8,16,2], depths=[2,2,2], heads=[2,4,8]):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        self.embed_dims = embed_dims\n",
    "        for i, d in enumerate(embed_dims):\n",
    "            in_ch = img_channels if i == 0 else embed_dims[i-1]\n",
    "            self.stages.append(nn.ModuleDict({\n",
    "                \"proj\": nn.Conv2d(in_ch, d, kernel_size=patch_sizes[i], stride=patch_sizes[i]),\n",
    "                \"blocks\": nn.ModuleList([TransformerBlock(d, heads[i]) for _ in range(depths[i])])\n",
    "            }))\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for s in self.stages:\n",
    "            x = s[\"proj\"](x)             # [B,D,h,w]\n",
    "            B, D, h, w = x.shape\n",
    "            x_seq = rearrange(x, \"b d h w -> (h w) b d\")\n",
    "            for blk in s[\"blocks\"]:\n",
    "                x_seq = blk(x_seq)\n",
    "            x = rearrange(x_seq, \"(h w) b d -> b d h w\", h=h, w=w)\n",
    "            skips.append(x)\n",
    "        return skips  # [low-res ... high-res]\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, embed_dims=[64,128,256], out_ch=2):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(embed_dims[2], embed_dims[1], 2, 2)\n",
    "        self.c1  = nn.Sequential(nn.Conv2d(embed_dims[1]*2, embed_dims[1], 3, padding=1), nn.ReLU(True),\n",
    "                                 nn.Conv2d(embed_dims[1], embed_dims[1], 3, padding=1), nn.ReLU(True))\n",
    "        self.up2 = nn.ConvTranspose2d(embed_dims[1], embed_dims[0], 2, 2)\n",
    "        self.c2  = nn.Sequential(nn.Conv2d(embed_dims[0]*2, embed_dims[0], 3, padding=1), nn.ReLU(True),\n",
    "                                 nn.Conv2d(embed_dims[0], embed_dims[0], 3, padding=1), nn.ReLU(True))\n",
    "        self.up3 = nn.ConvTranspose2d(embed_dims[0], embed_dims[0]//2, 2, 2)\n",
    "        self.c3  = nn.Sequential(nn.Conv2d(embed_dims[0]//2, embed_dims[0]//2, 3, padding=1), nn.ReLU(True))\n",
    "        self.out = nn.Conv2d(embed_dims[0]//2, out_ch, 1)\n",
    "    def forward(self, skips):\n",
    "        x = skips[-1]\n",
    "        x = self.up1(x)\n",
    "        s1 = F.interpolate(skips[1], size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, s1], dim=1); x = self.c1(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        s0 = F.interpolate(skips[0], size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, s0], dim=1); x = self.c2(x)\n",
    "\n",
    "        x = self.up3(x); x = self.c3(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class FlowGenerator(nn.Module):\n",
    "    \"\"\"Predicts flow to transform input between domains.\"\"\"\n",
    "    def __init__(self, img_channels=3, max_disp=48.0):\n",
    "        super().__init__()\n",
    "        self.enc = MultiStageTransformerEncoder(img_channels=img_channels)\n",
    "        self.dec = UNetDecoder()\n",
    "        self.max_disp = max_disp\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        skips = self.enc(x)\n",
    "        flow = self.dec(skips)\n",
    "        flow = F.interpolate(flow, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        # constrain displacement magnitude for stability\n",
    "        flow = torch.tanh(flow) * self.max_disp\n",
    "        # flow = flow * 10.0  # EXPERIMENT: 10x amplification for visibility\n",
    "        return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.421079Z",
     "iopub.status.busy": "2025-09-02T02:46:34.420901Z",
     "iopub.status.idle": "2025-09-02T02:46:34.441894Z",
     "shell.execute_reply": "2025-09-02T02:46:34.441321Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.421063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Doc3DDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_size=(448, 448), align_corners=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.align_corners = align_corners\n",
    "        \n",
    "        # Collect all image paths\n",
    "        self.image_paths = []\n",
    "        for i in [1,2,3,4]:\n",
    "            img_dir = os.path.join(root_dir, f'img_{i}', 'img')\n",
    "            for folder in os.listdir(img_dir):\n",
    "                folder_path = os.path.join(img_dir, folder)\n",
    "                if os.path.isdir(folder_path):\n",
    "                    for fname in os.listdir(folder_path):\n",
    "                        if fname.endswith('.png'):\n",
    "                            self.image_paths.append((folder, fname))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        folder, fname = self.image_paths[idx]\n",
    "        base_name = fname[:-4]  # remove .png extension\n",
    "        \n",
    "        # --- Load image ---\n",
    "        img_path = os.path.join(self.root_dir, f'img_{folder}', 'img', folder, fname)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.target_size)\n",
    "        image_tensor = torch.from_numpy(image).float().permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        # --- Load backward map ---\n",
    "        bm_path = os.path.join(self.root_dir, f'bm_{folder}', 'bm', folder, base_name + '.mat')\n",
    "        bm_data = h5.loadmat(bm_path)\n",
    "        backward_map = bm_data['bm'].astype(np.float32)  # (H,W,2), pixel coords\n",
    "\n",
    "        H_orig, W_orig = backward_map.shape[:2]\n",
    "        H_tgt, W_tgt = self.target_size\n",
    "\n",
    "        if (H_orig, W_orig) != (H_tgt, W_tgt):\n",
    "            # Resize & scale BM properly\n",
    "            scale_x = W_tgt / W_orig\n",
    "            scale_y = H_tgt / H_orig\n",
    "            bm_resized = np.zeros((H_tgt, W_tgt, 2), dtype=np.float32)\n",
    "            bm_resized[..., 0] = cv2.resize(backward_map[..., 0], (W_tgt, H_tgt)) * scale_x\n",
    "            bm_resized[..., 1] = cv2.resize(backward_map[..., 1], (W_tgt, H_tgt)) * scale_y\n",
    "            backward_map = bm_resized\n",
    "\n",
    "        # Pixel-space tensor\n",
    "        bm_pix = torch.from_numpy(backward_map).permute(2, 0, 1)  # (2,H,W)\n",
    "\n",
    "        # Normalized tensor for grid_sample\n",
    "        if self.align_corners:\n",
    "            norm_x = (bm_pix[0] / (W_tgt - 1)) * 2 - 1\n",
    "            norm_y = (bm_pix[1] / (H_tgt - 1)) * 2 - 1\n",
    "        else:\n",
    "            norm_x = (2 * bm_pix[0] + 1) / W_tgt - 1\n",
    "            norm_y = (2 * bm_pix[1] + 1) / H_tgt - 1\n",
    "        bm_norm = torch.stack([norm_x, norm_y], dim=0)  # (2,H,W)\n",
    "\n",
    "        return image_tensor, bm_pix.float(), bm_norm.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.443847Z",
     "iopub.status.busy": "2025-09-02T02:46:34.443629Z",
     "iopub.status.idle": "2025-09-02T02:46:34.462118Z",
     "shell.execute_reply": "2025-09-02T02:46:34.461503Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.443832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Loss Functions\n",
    "# ---------------------------\n",
    "def tv_loss(flow):\n",
    "    \"\"\"Total variation loss for smooth flow fields\"\"\"\n",
    "    dx = torch.abs(flow[:, :, :, 1:] - flow[:, :, :, :-1])\n",
    "    dy = torch.abs(flow[:, :, 1:, :] - flow[:, :, :-1, :])\n",
    "    return torch.mean(dx) + torch.mean(dy)\n",
    "\n",
    "def weak_jacobian_penalty(flow):\n",
    "    \"\"\"Weak Jacobian penalty to prevent fold-overs\"\"\"\n",
    "    B, C, H, W = flow.shape\n",
    "    # flow is already normalized to [-1,1], so gradients are consistent\n",
    "    grad_x = torch.gradient(flow[:, 0], dim=2)[0]  # ∂u/∂x\n",
    "    grad_y = torch.gradient(flow[:, 1], dim=1)[0]  # ∂v/∂y\n",
    "\n",
    "    # Approximate Jacobian determinant\n",
    "    jac_det = (1 + grad_x) * (1 + grad_y) - grad_x * grad_y\n",
    "    return torch.mean(torch.relu(-jac_det))  # penalize negative det (folds)\n",
    "\n",
    "# Perceptual Loss using VGG16\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer_ids=[3, 8, 15]):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=True).features.eval()\n",
    "        self.layers = nn.ModuleList([vgg[i] for i in layer_ids])\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "    def forward(self, pred, target):\n",
    "        loss = 0\n",
    "        x, y = pred, target\n",
    "        for layer in self.layers:\n",
    "            x, y = layer(x), layer(y)\n",
    "            loss += F.l1_loss(x, y)\n",
    "        return loss\n",
    "\n",
    "# class FlowLoss(nn.Module):\n",
    "#     def __init__(self, tv_weight=0.01, jac_weight=0.002):\n",
    "#         super().__init__()\n",
    "#         self.tv_weight = tv_weight\n",
    "#         self.jac_weight = jac_weight\n",
    "#         self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "#     def forward(self, pred_flow, target_flow):\n",
    "#         # L1 loss for flow regression\n",
    "#         l1_loss = self.l1_loss(pred_flow, target_flow)\n",
    "        \n",
    "#         # Regularization\n",
    "#         tv_loss_val = tv_loss(pred_flow)\n",
    "#         jac_loss_val = weak_jacobian_penalty(pred_flow)\n",
    "        \n",
    "#         total_loss = l1_loss + self.tv_weight * tv_loss_val + self.jac_weight * jac_loss_val\n",
    "#         return total_loss, l1_loss, tv_loss_val, jac_loss_val\n",
    "        \n",
    "class FlowLoss(nn.Module):\n",
    "    def __init__(self, tv_weight=0.01, jac_weight=0.001, perc_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.tv_weight = tv_weight\n",
    "        self.jac_weight = jac_weight\n",
    "        self.perc_weight = perc_weight\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.perc_loss = VGGPerceptualLoss()\n",
    "        \n",
    "    def forward(self, pred_flow, target_flow, pred_img=None, target_img=None):\n",
    "        # L1 loss for flow regression\n",
    "        l1_loss = self.l1_loss(pred_flow, target_flow)\n",
    "        # Regularization\n",
    "        tv_loss_val = tv_loss(pred_flow)\n",
    "        jac_loss_val = weak_jacobian_penalty(pred_flow)\n",
    "        # Perceptual loss\n",
    "        perc_loss_val = 0.0\n",
    "        if pred_img is not None and target_img is not None:\n",
    "            perc_loss_val = self.perc_loss(pred_img, target_img)\n",
    "            \n",
    "        total_loss = l1_loss + self.tv_weight*tv_loss_val + self.jac_weight*jac_loss_val + self.perc_weight*perc_loss_val\n",
    "        return total_loss, l1_loss, tv_loss_val, jac_loss_val, perc_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.462989Z",
     "iopub.status.busy": "2025-09-02T02:46:34.462756Z",
     "iopub.status.idle": "2025-09-02T02:46:34.479199Z",
     "shell.execute_reply": "2025-09-02T02:46:34.478509Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.462966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration\n",
    "# ---------------------------\n",
    "config = {\n",
    "    'data_root': \"/kaggle/input/doc3d-100k\",  # required\n",
    "    'batch_size': 8,\n",
    "    'epochs': 30,\n",
    "    'lr': 1e-3,\n",
    "    'save_dir': 'checkpoints',\n",
    "    'resume': None,\n",
    "    'max_disp': 48.0,\n",
    "    'tv_weight': 0.01,\n",
    "    'jac_weight': 0.001,\n",
    "    'perc_weight': 0.1,\n",
    "    'use_wandb': True\n",
    "}\n",
    "\n",
    "# Convert to namespace for dot notation access\n",
    "args = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.480214Z",
     "iopub.status.busy": "2025-09-02T02:46:34.480026Z",
     "iopub.status.idle": "2025-09-02T02:46:34.502289Z",
     "shell.execute_reply": "2025-09-02T02:46:34.501575Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.480196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# --- Visualization helper ---\n",
    "def prepare_tensor(tensor):\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        if tensor.ndim == 4:\n",
    "            tensor = tensor[0]  # remove batch dimension\n",
    "        if tensor.shape[0] in [1, 3, 4]:  # channels first\n",
    "            tensor = tensor.transpose(1, 2, 0)  # convert to channels last\n",
    "        return tensor\n",
    "    if tensor.requires_grad:\n",
    "        tensor = tensor.detach()\n",
    "    tensor = tensor.cpu().numpy()\n",
    "    if tensor.ndim == 4:\n",
    "        tensor = tensor[0]\n",
    "    if tensor.shape[0] in [1, 3, 4]:\n",
    "        tensor = tensor.transpose(1, 2, 0)\n",
    "    return tensor\n",
    "\n",
    "def visualize_flow(flow, save_path=None, verbose=True):\n",
    "    \"\"\"Visualize flow field as RGB image\"\"\"\n",
    "    try:\n",
    "        flow = flow.squeeze(0).cpu().numpy()\n",
    "    except:\n",
    "        flow = flow.squeeze(0)\n",
    "    \n",
    "    # Convert flow to HSV color representation\n",
    "    h, w = flow.shape[1:]\n",
    "    hsv = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Magnitude and angle\n",
    "    mag, ang = cv2.cartToPolar(flow[0], flow[1])\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert to BGR and save\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        cv2.imwrite(save_path, bgr)\n",
    "\n",
    "    # Create plot\n",
    "    if verbose:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(bgr)\n",
    "        plt.axis('off')\n",
    "        plt.title('Optical Flow Visualization')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return bgr\n",
    "\n",
    "def apply_bm_doc3d(img, bm_pix, align_corners=True, padding_mode=\"border\", verbose=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Warp an image using a backward map in pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        img: (B, C, H, W) tensor in [0,1], warped image\n",
    "        bm_pix: (B, 2, H, W) tensor in pixels, backward map (absolute coords)\n",
    "                bm_pix[:,0] = x pixel coords\n",
    "                bm_pix[:,1] = y pixel coords\n",
    "        align_corners: bool, matches normalization convention in grid_sample\n",
    "        padding_mode: str, 'border' or 'zeros'\n",
    "\n",
    "    Returns:\n",
    "        rectified: (B, C, H, W) tensor, unwarped image\n",
    "    \"\"\"\n",
    "    # if len(img.shape) == 3:\n",
    "    #     img = img.unsqueeze(0)\n",
    "    #     bm_pix = bm_pix.unsqueeze(0)\n",
    "        \n",
    "    B, C, H, W = img.shape\n",
    "\n",
    "    # convert pixel coords -> normalized [-1,1]\n",
    "    if align_corners:\n",
    "        norm_x = (bm_pix[:, 0, :, :] / (W - 1)) * 2 - 1\n",
    "        norm_y = (bm_pix[:, 1, :, :] / (H - 1)) * 2 - 1\n",
    "    else:\n",
    "        norm_x = (2 * bm_pix[:, 0, :, :] + 1) / W - 1\n",
    "        norm_y = (2 * bm_pix[:, 1, :, :] + 1) / H - 1\n",
    "\n",
    "    grid = torch.stack([norm_x, norm_y], dim=-1)  # (B,H,W,2)\n",
    "\n",
    "    rectified = F.grid_sample(\n",
    "        img, grid, mode=\"bilinear\",\n",
    "        padding_mode=padding_mode, align_corners=align_corners\n",
    "    )\n",
    "\n",
    "    # For PyTorch tensors with requires_grad\n",
    "    def prepare_tensor(tensor):\n",
    "        if tensor.requires_grad:\n",
    "            tensor = tensor.detach()\n",
    "        tensor = tensor.cpu().numpy()\n",
    "        if tensor.ndim == 4:\n",
    "            tensor = tensor[0]\n",
    "        if tensor.shape[0] in [1, 3, 4]:\n",
    "            tensor = tensor.transpose(1, 2, 0)\n",
    "        return tensor\n",
    "        \n",
    "    if verbose:\n",
    "        img_display = prepare_tensor(img)\n",
    "        rectified_display = prepare_tensor(rectified)\n",
    "        \n",
    "        f,axrr=plt.subplots(1,2)\n",
    "        for ax in axrr:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        axrr[0].imshow(img_display)\n",
    "        axrr[0].title.set_text('input')\n",
    "        axrr[1].imshow(rectified_display)\n",
    "        axrr[1].title.set_text('unwarped')\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        \n",
    "    return rectified\n",
    "\n",
    "def visualize_epoch_results(model, dataloader, device, epoch, max_batches=1):\n",
    "    model.eval()\n",
    "    images, target_flows, target_flows_norm = next(iter(dataloader))\n",
    "    print(images.shape)\n",
    "    # image_tensor, target_flow_tensor, target_flow_norm_tensor = dataloader[0]\n",
    "\n",
    "    # image_tensor, bm_tensor, _ = dataset[i]  # image: (3,H,W), bm: (2,H,W)\n",
    "    image_tensor = images[0].unsqueeze(0).to(device)   # -> (1,3,H,W)\n",
    "    target_flow_tensor    = target_flows[0].unsqueeze(0).to(device)      # -> (1,2,H,W)\n",
    "\n",
    "    # images = images.to(device)\n",
    "    # target_flows = target_flows.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_flows = model(image_tensor)\n",
    "\n",
    "    # B, _, H, W = pred_flows.shape\n",
    "\n",
    "    original = images[0]\n",
    "    target_flow = visualize_flow(target_flow_tensor, verbose=False)\n",
    "    pred_flow = visualize_flow(pred_flows, verbose=False)\n",
    "\n",
    "    # (1) Ground-truth dewarp\n",
    "    with torch.no_grad():\n",
    "        target_dewarped = apply_bm_doc3d(image_tensor, target_flow_tensor)\n",
    "        pred_dewarped = apply_bm_doc3d(image_tensor, pred_flows)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Row titles\n",
    "    axes[0, 0].set_ylabel('Target', fontsize=14, fontweight='bold', rotation=0, labelpad=40)\n",
    "    axes[1, 0].set_ylabel('Predicted', fontsize=14, fontweight='bold', rotation=0, labelpad=40)\n",
    "    \n",
    "    # Column titles\n",
    "    col_titles = ['Original', 'Dewarped', 'Flow']\n",
    "    for i, title in enumerate(col_titles):\n",
    "        axes[0, i].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot target row\n",
    "    axes[0, 0].imshow(prepare_tensor(original))\n",
    "    axes[0, 1].imshow(prepare_tensor(target_dewarped))\n",
    "    axes[0, 2].imshow(prepare_tensor(target_flow))\n",
    "    \n",
    "    # Plot predicted row\n",
    "    axes[1, 0].imshow(prepare_tensor(original))\n",
    "    axes[1, 1].imshow(prepare_tensor(pred_dewarped))\n",
    "    axes[1, 2].imshow(prepare_tensor(pred_flow))\n",
    "    \n",
    "    # Remove ticks from all axes\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"epoch_{epoch}_warps\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Save figure to bytes buffer\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.open(buf)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        f\"epoch_{epoch}_warps\": wandb.Image(pil_image, caption=\"Input | Warp | Flow\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.503250Z",
     "iopub.status.busy": "2025-09-02T02:46:34.503091Z",
     "iopub.status.idle": "2025-09-02T02:46:34.517075Z",
     "shell.execute_reply": "2025-09-02T02:46:34.516426Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.503237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, scaler, epoch, log_wandb):\n",
    "    model.train()\n",
    "    total_loss, total_l1, total_tv, total_jac = 0, 0, 0, 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images, target_flows, target_flows_norm) in enumerate(dataloader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target_flows = target_flows.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast():\n",
    "            pred_flows = model(images)   # (B,2,H,W), normalized\n",
    "            # TODO: to be modified for the perceptual loss calculation\n",
    "            loss, l1_loss, tv_loss_val, jac_loss_val, perc_loss_val = criterion(pred_flows, target_flows)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Accumulate stats\n",
    "        total_loss += loss.item()\n",
    "        total_l1 += l1_loss.item()\n",
    "        total_tv += tv_loss_val.item()\n",
    "        total_jac += jac_loss_val.item()\n",
    "        \n",
    "        # Progress logging\n",
    "        if batch_idx % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"[Epoch {epoch} | Batch {batch_idx}/{len(dataloader)}] \"\n",
    "                  f\"Loss={loss.item():.4f} | L1={l1_loss.item():.4f} | \"\n",
    "                  f\"TV={tv_loss_val.item():.4f} | Jac={jac_loss_val.item():.4f} \"\n",
    "                  f\"| Time={elapsed:.1f}s\")\n",
    "            \n",
    "            if log_wandb and wandb.run is not None:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"batch_l1_loss\": l1_loss.item(),\n",
    "                    \"batch_tv_loss\": tv_loss_val.item(),\n",
    "                    \"batch_jac_loss\": jac_loss_val.item(),\n",
    "                    \"batch_idx\": batch_idx + epoch * len(dataloader)\n",
    "                })\n",
    "    \n",
    "    n = len(dataloader)\n",
    "    return total_loss/n, total_l1/n, total_tv/n, total_jac/n\n",
    "\n",
    "# ---------------------------\n",
    "# Validation Function\n",
    "# ---------------------------\n",
    "def validate(model, dataloader, criterion, device, epoch, log_wandb):\n",
    "    model.eval()\n",
    "    total_loss, total_l1, total_tv, total_jac = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, target_flows, target_flows_norm in dataloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            target_flows = target_flows.to(device, non_blocking=True)\n",
    "            \n",
    "            pred_flows = model(images)\n",
    "            loss, l1_loss, tv_loss_val, jac_loss_val, perc_loss_val = criterion(pred_flows, target_flows)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_l1 += l1_loss.item()\n",
    "            total_tv += tv_loss_val.item()\n",
    "            total_jac += jac_loss_val.item()\n",
    "    \n",
    "    n = len(dataloader)\n",
    "\n",
    "    if log_wandb and wandb.run is not None:\n",
    "        visualize_epoch_results(model, dataloader, device, epoch+1)\n",
    "    \n",
    "    return total_loss/n, total_l1/n, total_tv/n, total_jac/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T02:46:34.518002Z",
     "iopub.status.busy": "2025-09-02T02:46:34.517720Z",
     "iopub.status.idle": "2025-09-02T02:46:34.537997Z",
     "shell.execute_reply": "2025-09-02T02:46:34.537494Z",
     "shell.execute_reply.started": "2025-09-02T02:46:34.517977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Set your API key and login to W&B\n",
    "WANDB_API_KEY = \"YOUR_WANDDB_API_KEY\"\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-02T02:48:28.635Z",
     "iopub.execute_input": "2025-09-02T02:46:34.538840Z",
     "iopub.status.busy": "2025-09-02T02:46:34.538644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Setup\n",
    "# -----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize wandb\n",
    "if args.use_wandb:\n",
    "    wandb.init(\n",
    "        project='supervised-dewarping-project', \n",
    "        config=args,\n",
    "        name='doc3d-20k-kaggle-scheduler'\n",
    "    )\n",
    "\n",
    "# Model\n",
    "model = FlowGenerator(max_disp=args.max_disp).to(device)\n",
    "\n",
    "# Resume checkpoint\n",
    "start_epoch = 0\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f'Resumed from epoch {start_epoch}')\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset / Dataloader\n",
    "# -----------------------------\n",
    "dataset = Doc3DDataset(root_dir=args.data_root, target_size=(448, 448))\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Validation size:\", val_size)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer + Scheduler / Loss\n",
    "# -----------------------------\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "criterion = FlowLoss(tv_weight=args.tv_weight, jac_weight=args.jac_weight)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr,\n",
    "    steps_per_epoch=100, epochs=args.epochs)\n",
    "scaler = GradScaler()\n",
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{args.epochs}')\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_l1, train_tv, train_jac = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, scaler, epoch, args.use_wandb\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_l1, val_tv, val_jac = validate(model, val_loader, criterion, device, epoch, args.use_wandb)\n",
    "\n",
    "    print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    print(f'  Train L1: {train_l1:.4f} | TV: {train_tv:.4f} | Jac: {train_jac:.4f}')\n",
    "    print(f'  Val L1: {val_l1:.4f}')\n",
    "\n",
    "    # wandb logging\n",
    "    if args.use_wandb:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_l1_loss': train_l1,\n",
    "            'train_tv_loss': train_tv,\n",
    "            'train_jac_loss': train_jac,\n",
    "            'val_loss': val_loss,\n",
    "            'val_l1_loss': val_l1,\n",
    "            'val_tv_loss': train_tv,\n",
    "            'val_jac_loss': train_jac,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(args.save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(args.save_dir, 'best_model.pth'))\n",
    "        print(f'New best model saved with val loss: {val_loss:.4f}')\n",
    "\n",
    "print('Training completed!')\n",
    "if args.use_wandb:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait, I forgot about the existence of the original Doc3D demo code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-02T02:48:28.635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# From dataset\n",
    "i = 0\n",
    "image_tensor, bm_tensor, _ = dataset[i]  # image: (3,H,W), bm: (2,H,W)\n",
    "image_tensor = image_tensor.unsqueeze(0)   # -> (1,3,H,W)\n",
    "bm_tensor    = bm_tensor.unsqueeze(0)      # -> (1,2,H,W)\n",
    "\n",
    "# (1) Ground-truth dewarp\n",
    "with torch.no_grad():\n",
    "    rectified_gt = apply_bm_doc3d(image_tensor, bm_tensor, verbose=True, save_path=\"dewarping_with_target_flow.png\")\n",
    "\n",
    "# (2) Model prediction\n",
    "pred_bm_pix = model(image_tensor.to(device))  # assume model outputs (B,2,H,W) in pixels\n",
    "rectified_pred = apply_bm_doc3d(image_tensor.to(device), pred_bm_pix.to(device), verbose=True, save_path=\"dewarping_with_predicted_flow.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-02T02:48:28.635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_flow(bm_tensor, \"target_flow.png\")\n",
    "visualize_flow(pred_bm_pix.to('cpu').detach().numpy(), \"predicted_flow_10ep.png\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8137752,
     "sourceId": 12935457,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
