{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e21eb81",
   "metadata": {
    "papermill": {
     "duration": 0.007402,
     "end_time": "2025-09-12T18:22:48.841349",
     "exception": false,
     "start_time": "2025-09-12T18:22:48.833947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760234e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:22:48.855705Z",
     "iopub.status.busy": "2025-09-12T18:22:48.855369Z",
     "iopub.status.idle": "2025-09-12T18:22:58.002648Z",
     "shell.execute_reply": "2025-09-12T18:22:58.001447Z"
    },
    "papermill": {
     "duration": 9.156561,
     "end_time": "2025-09-12T18:22:58.004627",
     "exception": false,
     "start_time": "2025-09-12T18:22:48.848066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\r\n",
      "The following additional packages will be installed:\r\n",
      "  libarchive-dev libleptonica-dev\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libarchive-dev libleptonica-dev libtesseract-dev\r\n",
      "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\r\n",
      "Need to get 3,743 kB of archives.\r\n",
      "After this operation, 16.0 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.5 [581 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\r\n",
      "Fetched 3,743 kB in 1s (3,925 kB/s)\r\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libarchive-dev:amd64.\r\n",
      "(Reading database ... 128663 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libleptonica-dev.\r\n",
      "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libleptonica-dev (1.82.0-3build1) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libtesseract-dev:amd64.\r\n",
      "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Setting up libleptonica-dev (1.82.0-3build1) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\r\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Processing triggers for man-db (2.10.2-1) ...\r\n",
      "\r\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install tesseract-ocr libtesseract-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3900a9d4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-12T18:22:58.024534Z",
     "iopub.status.busy": "2025-09-12T18:22:58.024145Z",
     "iopub.status.idle": "2025-09-12T18:23:05.713804Z",
     "shell.execute_reply": "2025-09-12T18:23:05.712471Z"
    },
    "papermill": {
     "duration": 7.702115,
     "end_time": "2025-09-12T18:23:05.715746",
     "exception": false,
     "start_time": "2025-09-12T18:22:58.013631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdf5storage\r\n",
      "  Downloading hdf5storage-0.2.1-py3-none-any.whl.metadata (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting Levenshtein\r\n",
      "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n",
      "Collecting jiwer\r\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: h5py>=3.9 in /usr/local/lib/python3.11/dist-packages (from hdf5storage) (3.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.11/dist-packages (from hdf5storage) (1.26.4)\r\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\r\n",
      "  Downloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26->hdf5storage) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26->hdf5storage) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26->hdf5storage) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26->hdf5storage) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26->hdf5storage) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26->hdf5storage) (2024.2.0)\r\n",
      "Downloading hdf5storage-0.2.1-py3-none-any.whl (75 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\r\n",
      "Downloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, jiwer, hdf5storage\r\n",
      "Successfully installed Levenshtein-0.27.1 hdf5storage-0.2.1 jiwer-4.0.0 rapidfuzz-3.14.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install hdf5storage Levenshtein jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6aec0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:23:05.736411Z",
     "iopub.status.busy": "2025-09-12T18:23:05.736076Z",
     "iopub.status.idle": "2025-09-12T18:24:50.215437Z",
     "shell.execute_reply": "2025-09-12T18:24:50.213931Z"
    },
    "papermill": {
     "duration": 104.49226,
     "end_time": "2025-09-12T18:24:50.217705",
     "exception": false,
     "start_time": "2025-09-12T18:23:05.725445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/allansdefreitas/yolov10.git\n",
    "!pip install -q supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db36f0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:24:50.291467Z",
     "iopub.status.busy": "2025-09-12T18:24:50.291099Z",
     "iopub.status.idle": "2025-09-12T18:24:51.954495Z",
     "shell.execute_reply": "2025-09-12T18:24:51.953374Z"
    },
    "papermill": {
     "duration": 1.703376,
     "end_time": "2025-09-12T18:24:51.956375",
     "exception": false,
     "start_time": "2025-09-12T18:24:50.252999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-12 18:24:50--  https://github.com/moured/YOLOv10-Document-Layout-Analysis/releases/download/doclaynet_weights/yolov10x_best.pt\r\n",
      "Resolving github.com (github.com)... 140.82.121.3\r\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/809399250/e52eefec-ac07-4944-997c-59e48e23474b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-12T19%3A15%3A19Z&rscd=attachment%3B+filename%3Dyolov10x_best.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-12T18%3A15%3A10Z&ske=2025-09-12T19%3A15%3A19Z&sks=b&skv=2018-11-09&sig=Werjo%2B080si%2B1IQDjMG2eOSFvkHTK7DyQV2B6UgTsak%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NzcwMTc5MCwibmJmIjoxNzU3NzAxNDkwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.dOVy1m4k3ZYRZMCR33Haip096id1kDqmEiZgP32yoZs&response-content-disposition=attachment%3B%20filename%3Dyolov10x_best.pt&response-content-type=application%2Foctet-stream [following]\r\n",
      "--2025-09-12 18:24:50--  https://release-assets.githubusercontent.com/github-production-release-asset/809399250/e52eefec-ac07-4944-997c-59e48e23474b?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-12T19%3A15%3A19Z&rscd=attachment%3B+filename%3Dyolov10x_best.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-12T18%3A15%3A10Z&ske=2025-09-12T19%3A15%3A19Z&sks=b&skv=2018-11-09&sig=Werjo%2B080si%2B1IQDjMG2eOSFvkHTK7DyQV2B6UgTsak%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NzcwMTc5MCwibmJmIjoxNzU3NzAxNDkwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.dOVy1m4k3ZYRZMCR33Haip096id1kDqmEiZgP32yoZs&response-content-disposition=attachment%3B%20filename%3Dyolov10x_best.pt&response-content-type=application%2Foctet-stream\r\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\r\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 64133947 (61M) [application/octet-stream]\r\n",
      "Saving to: ‘yolov10x_best.pt’\r\n",
      "\r\n",
      "yolov10x_best.pt    100%[===================>]  61.16M  51.4MB/s    in 1.2s    \r\n",
      "\r\n",
      "2025-09-12 18:24:51 (51.4 MB/s) - ‘yolov10x_best.pt’ saved [64133947/64133947]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/moured/YOLOv10-Document-Layout-Analysis/releases/download/doclaynet_weights/yolov10x_best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeecf196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:24:52.027724Z",
     "iopub.status.busy": "2025-09-12T18:24:52.027373Z",
     "iopub.status.idle": "2025-09-12T18:24:57.139254Z",
     "shell.execute_reply": "2025-09-12T18:24:57.138109Z"
    },
    "papermill": {
     "duration": 5.149941,
     "end_time": "2025-09-12T18:24:57.141263",
     "exception": false,
     "start_time": "2025-09-12T18:24:51.991322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reportlab\r\n",
      "  Downloading reportlab-4.4.3-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.2.1)\r\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab) (3.4.2)\r\n",
      "Downloading reportlab-4.4.3-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: reportlab\r\n",
      "Successfully installed reportlab-4.4.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba6fa7",
   "metadata": {
    "papermill": {
     "duration": 0.155984,
     "end_time": "2025-09-12T18:24:57.334047",
     "exception": false,
     "start_time": "2025-09-12T18:24:57.178063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model & Inference Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2eaee",
   "metadata": {
    "papermill": {
     "duration": 0.035374,
     "end_time": "2025-09-12T18:24:57.404949",
     "exception": false,
     "start_time": "2025-09-12T18:24:57.369575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dewarping Model Architecture (Flow Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bd6e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:24:57.479974Z",
     "iopub.status.busy": "2025-09-12T18:24:57.479631Z",
     "iopub.status.idle": "2025-09-12T18:25:12.293449Z",
     "shell.execute_reply": "2025-09-12T18:25:12.292501Z"
    },
    "papermill": {
     "duration": 14.853891,
     "end_time": "2025-09-12T18:25:12.295300",
     "exception": false,
     "start_time": "2025-09-12T18:24:57.441409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import hdf5storage as h5\n",
    "import cv2\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import time\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import wandb  # Optional for logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model Architecture (Your Transformer+U-Net)\n",
    "# ---------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4.0, p=0.0):\n",
    "        super().__init__()\n",
    "        self.n1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False)\n",
    "        self.n2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)), nn.GELU(), nn.Linear(int(dim*mlp_ratio), dim)\n",
    "        )\n",
    "    def forward(self, x):  # x: [HW,B,D]\n",
    "        h = self.n1(x)\n",
    "        a, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + a\n",
    "        x = x + self.mlp(self.n2(x))\n",
    "        return x\n",
    "\n",
    "class MultiStageTransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_channels=3, embed_dims=[64,128,256], patch_sizes=[8,16,2], depths=[2,2,2], heads=[2,4,8]):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        self.embed_dims = embed_dims\n",
    "        for i, d in enumerate(embed_dims):\n",
    "            in_ch = img_channels if i == 0 else embed_dims[i-1]\n",
    "            self.stages.append(nn.ModuleDict({\n",
    "                \"proj\": nn.Conv2d(in_ch, d, kernel_size=patch_sizes[i], stride=patch_sizes[i]),\n",
    "                \"blocks\": nn.ModuleList([TransformerBlock(d, heads[i]) for _ in range(depths[i])])\n",
    "            }))\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for s in self.stages:\n",
    "            x = s[\"proj\"](x)             # [B,D,h,w]\n",
    "            B, D, h, w = x.shape\n",
    "            x_seq = rearrange(x, \"b d h w -> (h w) b d\")\n",
    "            for blk in s[\"blocks\"]:\n",
    "                x_seq = blk(x_seq)\n",
    "            x = rearrange(x_seq, \"(h w) b d -> b d h w\", h=h, w=w)\n",
    "            skips.append(x)\n",
    "        return skips  # [low-res ... high-res]\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, embed_dims=[64,128,256], out_ch=2):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(embed_dims[2], embed_dims[1], 2, 2)\n",
    "        self.c1  = nn.Sequential(nn.Conv2d(embed_dims[1]*2, embed_dims[1], 3, padding=1), nn.ReLU(True),\n",
    "                                 nn.Conv2d(embed_dims[1], embed_dims[1], 3, padding=1), nn.ReLU(True))\n",
    "        self.up2 = nn.ConvTranspose2d(embed_dims[1], embed_dims[0], 2, 2)\n",
    "        self.c2  = nn.Sequential(nn.Conv2d(embed_dims[0]*2, embed_dims[0], 3, padding=1), nn.ReLU(True),\n",
    "                                 nn.Conv2d(embed_dims[0], embed_dims[0], 3, padding=1), nn.ReLU(True))\n",
    "        self.up3 = nn.ConvTranspose2d(embed_dims[0], embed_dims[0]//2, 2, 2)\n",
    "        self.c3  = nn.Sequential(nn.Conv2d(embed_dims[0]//2, embed_dims[0]//2, 3, padding=1), nn.ReLU(True))\n",
    "        self.out = nn.Conv2d(embed_dims[0]//2, out_ch, 1)\n",
    "    def forward(self, skips):\n",
    "        x = skips[-1]\n",
    "        x = self.up1(x)\n",
    "        s1 = F.interpolate(skips[1], size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, s1], dim=1); x = self.c1(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        s0 = F.interpolate(skips[0], size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, s0], dim=1); x = self.c2(x)\n",
    "\n",
    "        x = self.up3(x); x = self.c3(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class FlowGenerator(nn.Module):\n",
    "    \"\"\"Predicts flow to transform input between domains.\"\"\"\n",
    "    def __init__(self, img_channels=3, max_disp=48.0):\n",
    "        super().__init__()\n",
    "        self.enc = MultiStageTransformerEncoder(img_channels=img_channels)\n",
    "        self.dec = UNetDecoder()\n",
    "        self.max_disp = max_disp\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        skips = self.enc(x)\n",
    "        flow = self.dec(skips)\n",
    "        flow = F.interpolate(flow, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        # constrain displacement magnitude for stability\n",
    "        # flow = torch.tanh(flow) * self.max_disp\n",
    "        flow = flow * 10.0  # EXPERIMENT: 10x amplification for visibility\n",
    "        return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ba067",
   "metadata": {
    "papermill": {
     "duration": 0.035829,
     "end_time": "2025-09-12T18:25:12.367859",
     "exception": false,
     "start_time": "2025-09-12T18:25:12.332030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dewarping inference funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041c0cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:12.439521Z",
     "iopub.status.busy": "2025-09-12T18:25:12.439149Z",
     "iopub.status.idle": "2025-09-12T18:25:12.453183Z",
     "shell.execute_reply": "2025-09-12T18:25:12.452278Z"
    },
    "papermill": {
     "duration": 0.052159,
     "end_time": "2025-09-12T18:25:12.454728",
     "exception": false,
     "start_time": "2025-09-12T18:25:12.402569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_bm_doc3d(img, bm_pix, align_corners=True, padding_mode=\"border\", verbose=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Warp an image using a backward map in pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        img: (B, C, H, W) tensor in [0,1], warped image\n",
    "        bm_pix: (B, 2, H, W) tensor in pixels, backward map (absolute coords)\n",
    "                bm_pix[:,0] = x pixel coords\n",
    "                bm_pix[:,1] = y pixel coords\n",
    "        align_corners: bool, matches normalization convention in grid_sample\n",
    "        padding_mode: str, 'border' or 'zeros'\n",
    "\n",
    "    Returns:\n",
    "        rectified: (B, C, H, W) tensor, unwarped image\n",
    "    \"\"\"        \n",
    "    B, C, H, W = img.shape\n",
    "\n",
    "    # convert pixel coords -> normalized [-1,1]\n",
    "    if align_corners:\n",
    "        norm_x = (bm_pix[:, 0, :, :] / (W - 1)) * 2 - 1\n",
    "        norm_y = (bm_pix[:, 1, :, :] / (H - 1)) * 2 - 1\n",
    "    else:\n",
    "        norm_x = (2 * bm_pix[:, 0, :, :] + 1) / W - 1\n",
    "        norm_y = (2 * bm_pix[:, 1, :, :] + 1) / H - 1\n",
    "\n",
    "    grid = torch.stack([norm_x, norm_y], dim=-1)  # (B,H,W,2)\n",
    "\n",
    "    rectified = F.grid_sample(\n",
    "        img, grid, mode=\"bilinear\",\n",
    "        padding_mode=padding_mode, align_corners=align_corners\n",
    "    )\n",
    "        \n",
    "    if verbose:\n",
    "        img_display = prepare_tensor(img)\n",
    "        rectified_display = prepare_tensor(rectified)\n",
    "        \n",
    "        f,axrr=plt.subplots(1,2)\n",
    "        for ax in axrr:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        axrr[0].imshow(img_display)\n",
    "        axrr[0].title.set_text('input')\n",
    "        axrr[1].imshow(rectified_display)\n",
    "        axrr[1].title.set_text('unwarped')\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        \n",
    "    return rectified\n",
    "\n",
    "\n",
    "def scale_flow_to_resolution(flow_low_res, low_res, high_res):\n",
    "    \"\"\"\n",
    "    Scale flow from low resolution to high resolution\n",
    "    flow_low_res: [1, 2, H_low, W_low]\n",
    "    low_res: (H_low, W_low)\n",
    "    high_res: (H_high, W_high)\n",
    "    \"\"\"\n",
    "    if low_res == high_res:\n",
    "        return flow_low_res\n",
    "        \n",
    "    H_low, W_low = low_res\n",
    "    H_high, W_high = high_res\n",
    "    \n",
    "    # Scale factors\n",
    "    scale_x = W_high / W_low\n",
    "    scale_y = H_high / H_low\n",
    "    \n",
    "    # Resize flow\n",
    "    flow_high_res = F.interpolate(flow_low_res, size=(H_high, W_high), \n",
    "                                 mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Scale flow values\n",
    "    flow_high_res[:, 0, :, :] *= scale_x  # x coordinates\n",
    "    flow_high_res[:, 1, :, :] *= scale_y  # y coordinates\n",
    "    \n",
    "    return flow_high_res\n",
    "    \n",
    "    \n",
    "def dewarp_high_res_scaled(model, high_res_img_path, device, out_path=None, target_size=(448, 448)):\n",
    "    \"\"\"\n",
    "    (Main inference function)\n",
    "    Dewarp by scaling flow to high resolution\n",
    "    \"\"\"\n",
    "    # Load high-res image\n",
    "    high_res_img = cv2.imread(high_res_img_path)\n",
    "    high_res_img = cv2.cvtColor(high_res_img, cv2.COLOR_BGR2RGB)\n",
    "    original_size = high_res_img.shape[:2]  # (H, W)\n",
    "    \n",
    "    # Resize to training size for prediction\n",
    "    img_resized = cv2.resize(high_res_img, target_size)\n",
    "    img_tensor = torch.from_numpy(img_resized).float().permute(2, 0, 1) / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict flow on resized image\n",
    "    with torch.no_grad():\n",
    "        flow_low_res = model(img_tensor)  # [1, 2, 448, 448]\n",
    "    \n",
    "    # Scale flow to original resolution\n",
    "    flow_high_res = scale_flow_to_resolution(flow_low_res, target_size, original_size)\n",
    "    \n",
    "    # Prepare high-res image tensor\n",
    "    img_high_res_tensor = torch.from_numpy(high_res_img).float().permute(2, 0, 1) / 255.0\n",
    "    img_high_res_tensor = img_high_res_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Apply scaled flow to high-res image\n",
    "    dewarped_high_res = apply_bm_doc3d(img_high_res_tensor, flow_high_res, align_corners=True)\n",
    "    \n",
    "    # Save\n",
    "    if out_path is not None:\n",
    "        save_dewarped_result(dewarped_high_res, out_path)\n",
    "\n",
    "    return dewarped_high_res, high_res_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b475be2",
   "metadata": {
    "papermill": {
     "duration": 0.036689,
     "end_time": "2025-09-12T18:25:12.527814",
     "exception": false,
     "start_time": "2025-09-12T18:25:12.491125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualzation helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f328c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:12.603271Z",
     "iopub.status.busy": "2025-09-12T18:25:12.602404Z",
     "iopub.status.idle": "2025-09-12T18:25:12.617032Z",
     "shell.execute_reply": "2025-09-12T18:25:12.616260Z"
    },
    "papermill": {
     "duration": 0.053588,
     "end_time": "2025-09-12T18:25:12.618493",
     "exception": false,
     "start_time": "2025-09-12T18:25:12.564905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def prepare_tensor(tensor):\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        if tensor.ndim == 4:\n",
    "            tensor = tensor[0]  # remove batch dimension\n",
    "        if tensor.shape[0] in [1, 3, 4]:  # channels first\n",
    "            tensor = tensor.transpose(1, 2, 0)  # convert to channels last\n",
    "        return tensor\n",
    "    if tensor.requires_grad:\n",
    "        tensor = tensor.detach()\n",
    "    tensor = tensor.cpu().numpy()\n",
    "    if tensor.ndim == 4:\n",
    "        tensor = tensor[0]\n",
    "    if tensor.shape[0] in [1, 3, 4]:\n",
    "        tensor = tensor.transpose(1, 2, 0)\n",
    "    return tensor\n",
    "\n",
    "def visualize_flow(flow, save_path=None, verbose=True):\n",
    "    \"\"\"Visualize flow field as RGB image\"\"\"\n",
    "    try:\n",
    "        flow = flow.squeeze(0).cpu().numpy()\n",
    "    except:\n",
    "        flow = flow.squeeze(0)\n",
    "    \n",
    "    # Convert flow to HSV color representation\n",
    "    h, w = flow.shape[1:]\n",
    "    hsv = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Magnitude and angle\n",
    "    mag, ang = cv2.cartToPolar(flow[0], flow[1])\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert to BGR and save\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        cv2.imwrite(save_path, bgr)\n",
    "\n",
    "    # Create plot\n",
    "    if verbose:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(bgr)\n",
    "        plt.axis('off')\n",
    "        plt.title('Optical Flow Visualization')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return bgr\n",
    "\n",
    "def save_dewarped_result(dewarped_tensor, out_path, clip=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Save a dewarped image tensor to file\n",
    "    \n",
    "    Args:\n",
    "        dewarped_tensor: torch.Tensor of shape [1, 3, H, W] or [3, H, W] in range [0, 1]\n",
    "        out_path: path to save the image\n",
    "        clip: whether to clip values to [0, 1] range\n",
    "        verbose: whether to print debug information\n",
    "    \"\"\"\n",
    "    # Ensure we're working with CPU numpy array\n",
    "    if hasattr(dewarped_tensor, 'detach'):\n",
    "        dewarped_tensor = dewarped_tensor.detach()\n",
    "    if hasattr(dewarped_tensor, 'cpu'):\n",
    "        dewarped_tensor = dewarped_tensor.cpu()\n",
    "    if hasattr(dewarped_tensor, 'numpy'):\n",
    "        dewarped_tensor = dewarped_tensor.numpy()\n",
    "    \n",
    "    # Handle different tensor shapes\n",
    "    if dewarped_tensor.ndim == 4:  # [B, C, H, W]\n",
    "        dewarped_tensor = dewarped_tensor[0]  # Take first batch\n",
    "    if dewarped_tensor.ndim == 3:  # [C, H, W]\n",
    "        dewarped_tensor = dewarped_tensor.transpose(1, 2, 0)  # [H, W, C]\n",
    "    \n",
    "    # Clip values to valid range\n",
    "    if clip:\n",
    "        dewarped_tensor = np.clip(dewarped_tensor, 0, 1)\n",
    "    \n",
    "    # Convert to 8-bit and change channel order for OpenCV\n",
    "    dewarped_image = (dewarped_tensor * 255).astype(np.uint8)\n",
    "    dewarped_image = cv2.cvtColor(dewarped_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Save image\n",
    "    success = cv2.imwrite(out_path, dewarped_image)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Saved dewarped image to: {out_path}\")\n",
    "        print(f\"Image shape: {dewarped_image.shape}\")\n",
    "        print(f\"Value range: [{dewarped_tensor.min():.3f}, {dewarped_tensor.max():.3f}]\")\n",
    "        print(f\"Save successful: {success}\")\n",
    "    \n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9114d",
   "metadata": {
    "papermill": {
     "duration": 0.034698,
     "end_time": "2025-09-12T18:25:12.688339",
     "exception": false,
     "start_time": "2025-09-12T18:25:12.653641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OCR Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989ee68b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:12.760195Z",
     "iopub.status.busy": "2025-09-12T18:25:12.759780Z",
     "iopub.status.idle": "2025-09-12T18:25:14.684660Z",
     "shell.execute_reply": "2025-09-12T18:25:14.683489Z"
    },
    "papermill": {
     "duration": 1.962851,
     "end_time": "2025-09-12T18:25:14.686245",
     "exception": false,
     "start_time": "2025-09-12T18:25:12.723394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import Levenshtein as lv\n",
    "from jiwer import cer\n",
    "\n",
    "\n",
    "# TODO: extend to add the ability for bounding boxes extraction\n",
    "def extract_OCR_text(filepath, output_txt=None):\n",
    "    with Image.open(filepath) as img:\n",
    "        # OCR_text = pytesseract.image_to_string(img, config=\"--oem 1\")\n",
    "        OCR_text = pytesseract.image_to_string(img)\n",
    "\n",
    "    if output_txt is not None:\n",
    "        with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "            f.write(OCR_text)\n",
    "    return OCR_text\n",
    "\n",
    "def calculate_OCR_metrics(GT_text, OCR_text):\n",
    "    CER = cer(GT_text, OCR_text)\n",
    "    ED = lv.distance(OCR_text, GT_text)\n",
    "    return {\n",
    "        'CER': CER,\n",
    "        'ED': ED\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972de48",
   "metadata": {
    "papermill": {
     "duration": 0.034656,
     "end_time": "2025-09-12T18:25:14.756224",
     "exception": false,
     "start_time": "2025-09-12T18:25:14.721568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Layout Analysis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8702ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:14.827562Z",
     "iopub.status.busy": "2025-09-12T18:25:14.826960Z",
     "iopub.status.idle": "2025-09-12T18:25:16.556528Z",
     "shell.execute_reply": "2025-09-12T18:25:16.555501Z"
    },
    "papermill": {
     "duration": 1.767144,
     "end_time": "2025-09-12T18:25:16.558287",
     "exception": false,
     "start_time": "2025-09-12T18:25:14.791143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from ultralytics import YOLOv10\n",
    "\n",
    "\n",
    "doc_layout_model = YOLOv10('yolov10x_best.pt')\n",
    "\n",
    "def detect_img_layout(path, verbose=False):\n",
    "    \"\"\"\n",
    "    names: {0: 'Caption', 1: 'Footnote', 2: 'Formula', 3: 'List-item', 4: 'Page-footer', 5: 'Page-header', \n",
    "            6: 'Picture', 7: 'Section-header', 8: 'Table', 9: 'Text', 10: 'Title'}\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    results = doc_layout_model(source=path, conf=0.2, iou=0.8)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "    if verbose == True:\n",
    "        bounding_box_annotator = sv.BoxAnnotator()\n",
    "        label_annotator = sv.LabelAnnotator()\n",
    "        \n",
    "        annotated_image = bounding_box_annotator.annotate(\n",
    "            scene=image, detections=detections)\n",
    "        annotated_image = label_annotator.annotate(\n",
    "            scene=annotated_image, detections=detections)\n",
    "    \n",
    "        sv.plot_image(annotated_image)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108eb92",
   "metadata": {
    "papermill": {
     "duration": 0.034618,
     "end_time": "2025-09-12T18:25:16.628382",
     "exception": false,
     "start_time": "2025-09-12T18:25:16.593764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing data for benchmark (DIR300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4324952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:16.700672Z",
     "iopub.status.busy": "2025-09-12T18:25:16.700071Z",
     "iopub.status.idle": "2025-09-12T18:25:17.155052Z",
     "shell.execute_reply": "2025-09-12T18:25:17.153794Z"
    },
    "papermill": {
     "duration": 0.493592,
     "end_time": "2025-09-12T18:25:17.156674",
     "exception": false,
     "start_time": "2025-09-12T18:25:16.663082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files selected for testing: 65\n",
      "All files exist\n"
     ]
    }
   ],
   "source": [
    "lis = range(1, 66)\n",
    "print(\"Number of files selected for testing:\", len(lis))\n",
    "\n",
    "# Create a dictionary with paths to images\n",
    "img_paths_dict = {}\n",
    "doc_type = 'crop'\n",
    "path = f'/kaggle/input/docunet/{doc_type}/{doc_type}/'\n",
    "img_paths_1 = [os.path.join(path, f\"{i}_1 copy.png\") for i in lis]\n",
    "img_paths_2 = [os.path.join(path, f\"{i}_2 copy.png\") for i in lis]\n",
    "img_paths_dict[doc_type] = img_paths_1 + img_paths_2\n",
    "\n",
    "# Check if all images exist\n",
    "for img_paths in img_paths_dict.values():\n",
    "    for path in img_paths:\n",
    "        assert os.path.exists(path), path\n",
    "        # if not os.path.exists(path):\n",
    "        #     print(path)\n",
    "print(\"All files exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166a5cb",
   "metadata": {
    "papermill": {
     "duration": 0.040031,
     "end_time": "2025-09-12T18:25:17.236183",
     "exception": false,
     "start_time": "2025-09-12T18:25:17.196152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a815e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:17.309991Z",
     "iopub.status.busy": "2025-09-12T18:25:17.309687Z",
     "iopub.status.idle": "2025-09-12T18:25:17.314713Z",
     "shell.execute_reply": "2025-09-12T18:25:17.313918Z"
    },
    "papermill": {
     "duration": 0.043549,
     "end_time": "2025-09-12T18:25:17.316114",
     "exception": false,
     "start_time": "2025-09-12T18:25:17.272565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'max_disp': 48.0,\n",
    "}\n",
    "\n",
    "ckpt_path_dict = {\n",
    "    'kaggle_5k_98ep': '/kaggle/input/supervised-dewarping-training/checkpoints/checkpoint_epoch_98.pth',\n",
    "    'cc_100k_8ep': '/kaggle/input/supervised_dewarping_model/pytorch/epoch-8/1/checkpoint_epoch_8.pth',\n",
    "    'cc_100k_24ep': '/kaggle/input/supervised_dewarping_model/pytorch/default-epoch-24/1/checkpoint_epoch_24.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db32b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:17.388832Z",
     "iopub.status.busy": "2025-09-12T18:25:17.388503Z",
     "iopub.status.idle": "2025-09-12T18:25:17.393970Z",
     "shell.execute_reply": "2025-09-12T18:25:17.392880Z"
    },
    "papermill": {
     "duration": 0.043306,
     "end_time": "2025-09-12T18:25:17.395267",
     "exception": false,
     "start_time": "2025-09-12T18:25:17.351961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu, max_disp: 48.0\n"
     ]
    }
   ],
   "source": [
    "device = config['device']\n",
    "max_disp = config['max_disp']\n",
    "\n",
    "print(f'Using device: {device}, max_disp: {max_disp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40fe519e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:17.472104Z",
     "iopub.status.busy": "2025-09-12T18:25:17.471759Z",
     "iopub.status.idle": "2025-09-12T18:25:20.093625Z",
     "shell.execute_reply": "2025-09-12T18:25:20.092726Z"
    },
    "papermill": {
     "duration": 2.663492,
     "end_time": "2025-09-12T18:25:20.095231",
     "exception": false,
     "start_time": "2025-09-12T18:25:17.431739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kaggle_5k_98ep', 'cc_100k_8ep', 'cc_100k_24ep'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict = {}\n",
    "\n",
    "for ckpt, path in ckpt_path_dict.items():\n",
    "    model = FlowGenerator(max_disp=max_disp).to(device)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    models_dict[ckpt] = model\n",
    "\n",
    "models_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f5d6e",
   "metadata": {
    "papermill": {
     "duration": 0.035633,
     "end_time": "2025-09-12T18:25:20.166580",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.130947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dewarping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7631bf73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:20.238993Z",
     "iopub.status.busy": "2025-09-12T18:25:20.238651Z",
     "iopub.status.idle": "2025-09-12T18:25:20.245863Z",
     "shell.execute_reply": "2025-09-12T18:25:20.245085Z"
    },
    "papermill": {
     "duration": 0.045461,
     "end_time": "2025-09-12T18:25:20.247322",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.201861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def process_and_zip_dewarping_results_simple(img_paths, models_dict, device, output_dir=\"dewarping_results\"):\n",
    "    \"\"\"\n",
    "    Simple version that saves files directly to output directory and creates zip.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    processed_files = []\n",
    "    \n",
    "    for warped_img_path in img_paths:\n",
    "        print(f\"Processing: {warped_img_path}\")\n",
    "        base_name = os.path.basename(warped_img_path)[:-4]\n",
    "        \n",
    "        for name, model in models_dict.items():\n",
    "            output_filename = f\"{base_name}-{name}.png\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Process and save the image\n",
    "            dewarped, high_res_img = dewarp_high_res_scaled(\n",
    "                model, warped_img_path, device, out_path=output_path\n",
    "            )\n",
    "            \n",
    "            processed_files.append(output_path)\n",
    "            print(f\"  Saved: {output_filename}\")\n",
    "\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "    \"\"\"Get a list of all file paths in directory recursively\"\"\"\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8500961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:20.319868Z",
     "iopub.status.busy": "2025-09-12T18:25:20.319196Z",
     "iopub.status.idle": "2025-09-12T18:25:20.323571Z",
     "shell.execute_reply": "2025-09-12T18:25:20.322525Z"
    },
    "papermill": {
     "duration": 0.04256,
     "end_time": "2025-09-12T18:25:20.325175",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.282615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Dewarping DocUNet...\")\n",
    "# dewarping_results_dir = \"dewarping_results-DocUNet\"\n",
    "# process_and_zip_dewarping_results_simple(img_paths_dict[\"crop\"], models_dict, device, output_dir=dewarping_results_dir)\n",
    "# !zip -r DocUNet-dewarping_results.zip dewarping_results-DocUNet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77641477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:20.400131Z",
     "iopub.status.busy": "2025-09-12T18:25:20.399757Z",
     "iopub.status.idle": "2025-09-12T18:25:20.404111Z",
     "shell.execute_reply": "2025-09-12T18:25:20.403217Z"
    },
    "papermill": {
     "duration": 0.043615,
     "end_time": "2025-09-12T18:25:20.405583",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.361968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dewarping_results_dir = \"/kaggle/input/dewarping-benchmarks-artifacts/DocUNet-dewarping_results/dewarping_results-DocUNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbef936",
   "metadata": {
    "papermill": {
     "duration": 0.036821,
     "end_time": "2025-09-12T18:25:20.478461",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.441640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extracting OCR texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3123d07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:20.551649Z",
     "iopub.status.busy": "2025-09-12T18:25:20.550771Z",
     "iopub.status.idle": "2025-09-12T18:25:20.556209Z",
     "shell.execute_reply": "2025-09-12T18:25:20.555485Z"
    },
    "papermill": {
     "duration": 0.043534,
     "end_time": "2025-09-12T18:25:20.557669",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.514135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ocr_to_txt(img_paths, output_dir=\"text_files\"):\n",
    "    \"\"\"\n",
    "    Extract OCR text for every image and save the output to txt file in the output_dir.\n",
    "    \"\"\" \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    processed_files = []\n",
    "    \n",
    "    for img_path in img_paths:\n",
    "        print(f\"Processing: {img_path}\")\n",
    "        \n",
    "        base_name = os.path.basename(img_path)[:-4]\n",
    "        output_filename = f\"{base_name}.txt\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        assert os.path.exists(img_path), img_path\n",
    "        \n",
    "        # text = extract_OCR_text(img_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fc6583b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:20.629567Z",
     "iopub.status.busy": "2025-09-12T18:25:20.629179Z",
     "iopub.status.idle": "2025-09-12T18:25:21.611039Z",
     "shell.execute_reply": "2025-09-12T18:25:21.610216Z"
    },
    "papermill": {
     "duration": 1.020056,
     "end_time": "2025-09-12T18:25:21.612778",
     "exception": false,
     "start_time": "2025-09-12T18:25:20.592722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_paths_dict[\"scan\"] = get_all_file_paths(\"/kaggle/input/docunet/scan\")\n",
    "img_paths_dict[\"dewarped\"] = get_all_file_paths(dewarping_results_dir)\n",
    "# img_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed1cef1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:21.685887Z",
     "iopub.status.busy": "2025-09-12T18:25:21.685047Z",
     "iopub.status.idle": "2025-09-12T18:25:21.695504Z",
     "shell.execute_reply": "2025-09-12T18:25:21.694464Z"
    },
    "papermill": {
     "duration": 0.048411,
     "end_time": "2025-09-12T18:25:21.696932",
     "exception": false,
     "start_time": "2025-09-12T18:25:21.648521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR texts for DIR-300 scan already exist.\n",
      "OCR texts for DIR-300 crop already exist.\n",
      "OCR texts for DIR-300 dewarped already exist.\n"
     ]
    }
   ],
   "source": [
    "artifacts_dir_path = \"/kaggle/input/dewarping-benchmarks-artifacts\"\n",
    "\n",
    "ocr_dir_names = {\n",
    "    \"scan\": \"DocUNet-scan_text_files\",\n",
    "    \"crop\": \"DocUNet-crop_text_files\",\n",
    "    \"dewarped\": \"DocUNet-dewarped_text_files\"\n",
    "}\n",
    "\n",
    "ocr_dir_paths = {}\n",
    "for doc_type, dir_name in ocr_dir_names.items():\n",
    "    if not os.path.exists(os.path.join(artifacts_dir_path, dir_name)):\n",
    "        print(f\"Extracting OCR texts for DocUNet {doc_type}...\")\n",
    "        # OCR-ize text\n",
    "        save_ocr_to_txt(img_paths_dict[doc_type], dir_name)  \n",
    "        \n",
    "        # Create a zip of {dir_name} directory and name it {dir_name}.zip\n",
    "        output_zip = f\"{dir_name}.zip\"\n",
    "        print(f\"Creating zip archive: {output_zip}\")\n",
    "        with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
    "            all_files = get_all_file_paths(dir_name)\n",
    "            for file in all_files:\n",
    "                zipf.write(file, os.path.basename(file))\n",
    "                \n",
    "        ocr_dir_paths[doc_type] = dir_name\n",
    "    else:\n",
    "        print(f\"OCR texts for DIR-300 {doc_type} already exist.\")\n",
    "        ocr_dir_paths[doc_type] = os.path.join(artifacts_dir_path, dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e9dc9",
   "metadata": {
    "papermill": {
     "duration": 0.035068,
     "end_time": "2025-09-12T18:25:21.767734",
     "exception": false,
     "start_time": "2025-09-12T18:25:21.732666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CER and ED for DocUNet benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2297a82c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:21.840348Z",
     "iopub.status.busy": "2025-09-12T18:25:21.840040Z",
     "iopub.status.idle": "2025-09-12T18:25:21.847824Z",
     "shell.execute_reply": "2025-09-12T18:25:21.846921Z"
    },
    "papermill": {
     "duration": 0.045782,
     "end_time": "2025-09-12T18:25:21.849302",
     "exception": false,
     "start_time": "2025-09-12T18:25:21.803520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source (original authors of the DIR300 dataset): https://github.com/fh2019ustc/DocGeoNet/blob/main/OCR_eval_DIR300.py\n",
    "\n",
    "def Levenshtein_Distance(str1, str2):\n",
    "    matrix = [[ i + j for j in range(len(str2) + 1)] for i in range(len(str1) + 1)]\n",
    "    for i in range(1, len(str1)+1):\n",
    "        for j in range(1, len(str2)+1):\n",
    "            if(str1[i-1] == str2[j-1]):\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1 \n",
    "            matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+d)\n",
    "\n",
    "    return matrix[len(str1)][len(str2)]\n",
    "\n",
    "def calculate_CER_ED_DIR300(GT_text, OCR_tex):\n",
    "    l1 = Levenshtein_Distance(GT_text, OCR_tex)\n",
    "    ed = l1\n",
    "    cer = l1 / len(GT_text) if len(GT_text) > 0 else 1.0  # Handle empty GT\n",
    "    return {\n",
    "        'CER': cer,\n",
    "        'ED': ed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6de49542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:21.922469Z",
     "iopub.status.busy": "2025-09-12T18:25:21.922120Z",
     "iopub.status.idle": "2025-09-12T18:25:21.931646Z",
     "shell.execute_reply": "2025-09-12T18:25:21.930887Z"
    },
    "papermill": {
     "duration": 0.047945,
     "end_time": "2025-09-12T18:25:21.933074",
     "exception": false,
     "start_time": "2025-09-12T18:25:21.885129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cer_ed_doc3d_benchmark(path_gt, path_ours, tail=\" copy\"):\n",
    "    # print(\"Number of gt text files:\", len(gt_txt_paths))\n",
    "    # print(\"Number of pred text files:\", len(pred_txt_paths))\n",
    "    # assert 2 * len(gt_txt_paths) == len(pred_txt_paths)\n",
    "\n",
    "    # cer_list = []\n",
    "    # ed_list = []\n",
    "\n",
    "    # gt_txt_paths, pred_txt_paths = sorted(gt_txt_paths), sorted(pred_txt_paths)\n",
    "    \n",
    "    # for gt_file, pred_file in zip(gt_txt_paths, pred_txt_paths):       \n",
    "    #     with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "    #         content_gt = f.read()\n",
    "    #     with open(pred_file, 'r', encoding='utf-8') as f:\n",
    "    #         content_pred = f.read()   \n",
    "                \n",
    "    #     # Calculate metrics\n",
    "    #     metrics = calculate_CER_ED_DIR300(content_gt, content_pred)\n",
    "\n",
    "\n",
    "    N=66\n",
    "    cer1=[]\n",
    "    cer2=[]\n",
    "    ed1=[]\n",
    "    ed2=[]\n",
    "    check=[0 for _ in range(N+1)]\n",
    "    # lis=[1,2,3,4,5,6,7,9,10,21,22,23,24,27,30,31,32,36,38,40,41,44,45,46,47,48,50,51,52,53]  # DocTr (Setting 1)\n",
    "    lis=[1,9,10,12,19,20,21,22,23,24,30,31,32,34,35,36,37,38,39,40,44,45,46,47,49] # DewarpNet (Setting 2)\n",
    "    for i in range(1,N):\n",
    "        if i not in lis:\n",
    "            continue\n",
    "        gt=path_gt+str(i)+'.txt'\n",
    "        img1=path_ours+str(i)+'_1' + tail + \".txt\"\n",
    "        img2=path_ours+str(i)+'_2' + tail + \".txt\"\n",
    "\n",
    "        with open(gt, 'r', encoding='utf-8') as f:\n",
    "            content_gt = f.read()\n",
    "        with open(img1, 'r', encoding='utf-8') as f:\n",
    "            content1 = f.read()\n",
    "        with open(img2, 'r', encoding='utf-8') as f:\n",
    "            content2 = f.read()\n",
    "        \n",
    "        metrics_1 = calculate_CER_ED_DIR300(content_gt, content1)\n",
    "        metrics_2 = calculate_CER_ED_DIR300(content_gt, content2)\n",
    "        \n",
    "        l1=metrics_1[\"ED\"]\n",
    "        l2=metrics_2[\"ED\"]\n",
    "        ed1.append(l1)\n",
    "        ed2.append(l2)\n",
    "        cer1.append(metrics_1[\"CER\"])\n",
    "        cer2.append(metrics_2[\"CER\"])\n",
    "        check[i]=cer1[-1]\n",
    "        \n",
    "    print('CER: ', (np.mean(cer1)+np.mean(cer2)) / 2.)\n",
    "    print('ED:  ', (np.mean(ed1)+np.mean(ed2)) / 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd1e9165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:22.008098Z",
     "iopub.status.busy": "2025-09-12T18:25:22.007380Z",
     "iopub.status.idle": "2025-09-12T18:25:22.011687Z",
     "shell.execute_reply": "2025-09-12T18:25:22.010829Z"
    },
    "papermill": {
     "duration": 0.043543,
     "end_time": "2025-09-12T18:25:22.013207",
     "exception": false,
     "start_time": "2025-09-12T18:25:21.969664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_texts_dir = \"/kaggle/input/dewarping-benchmarks-artifacts/DocUNet-scan_text_files/\"\n",
    "crop_texts_dir = \"/kaggle/input/dewarping-benchmarks-artifacts/DocUNet-crop_text_files/\"\n",
    "dewarped_texts_dir = \"/kaggle/input/dewarping-benchmarks-artifacts/DocUNet-dewarped_text_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ef05e",
   "metadata": {
    "papermill": {
     "duration": 0.035611,
     "end_time": "2025-09-12T18:25:22.084747",
     "exception": false,
     "start_time": "2025-09-12T18:25:22.049136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ground truth flat images vs the original warped images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5d59dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:25:22.158023Z",
     "iopub.status.busy": "2025-09-12T18:25:22.157158Z",
     "iopub.status.idle": "2025-09-12T18:30:00.857533Z",
     "shell.execute_reply": "2025-09-12T18:30:00.856540Z"
    },
    "papermill": {
     "duration": 278.773056,
     "end_time": "2025-09-12T18:30:00.893727",
     "exception": false,
     "start_time": "2025-09-12T18:25:22.120671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER:  0.5179986427950869\n",
      "ED:   2028.04\n"
     ]
    }
   ],
   "source": [
    "metrics_list = cer_ed_doc3d_benchmark(scan_texts_dir, crop_texts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bb59c",
   "metadata": {
    "papermill": {
     "duration": 0.034959,
     "end_time": "2025-09-12T18:30:00.964031",
     "exception": false,
     "start_time": "2025-09-12T18:30:00.929072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ground truth flat images vs dewarped images from 3 different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "431222d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:30:01.036236Z",
     "iopub.status.busy": "2025-09-12T18:30:01.035907Z",
     "iopub.status.idle": "2025-09-12T18:46:06.496818Z",
     "shell.execute_reply": "2025-09-12T18:46:06.495513Z"
    },
    "papermill": {
     "duration": 965.534074,
     "end_time": "2025-09-12T18:46:06.533463",
     "exception": false,
     "start_time": "2025-09-12T18:30:00.999389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model kaggle_5k_98ep\n",
      "CER:  0.5827080988652188\n",
      "ED:   2217.62\n",
      "Model cc_100k_8ep\n",
      "CER:  0.5787088120428401\n",
      "ED:   2189.88\n",
      "Model cc_100k_24ep\n",
      "CER:  0.5122600578570559\n",
      "ED:   1955.2800000000002\n"
     ]
    }
   ],
   "source": [
    "for model in models_dict.keys():\n",
    "    print(f\"Model {model}\")\n",
    "    cer_ed_doc3d_benchmark(scan_texts_dir, dewarped_texts_dir, f\" copy-{model}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8138261,
     "sourceId": 12865864,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8138187,
     "sourceId": 12962518,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8257313,
     "sourceId": 13040135,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8209047,
     "sourceId": 13041198,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 258930833,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 441464,
     "modelInstanceId": 423947,
     "sourceId": 557814,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 441464,
     "modelInstanceId": 423949,
     "sourceId": 557817,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1406.383325,
   "end_time": "2025-09-12T18:46:09.960103",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T18:22:43.576778",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
